# Serverless Architecture on AWS ‚ö°

Master serverless computing patterns and build highly scalable, cost-effective applications using AWS Lambda and serverless services.

## üéØ Learning Objectives

After completing this section, you'll be able to:
- Design and implement serverless architectures
- Build event-driven applications with AWS Lambda
- Create serverless APIs with API Gateway
- Implement data processing pipelines
- Apply serverless best practices and patterns

## üìö Prerequisites

**Required Completion**:
- [x] [AWS Fundamentals](../01-aws-fundamentals/) - Core AWS services
- [x] [Basic Web Application](../02-basic-web-application/) - Web application concepts
- [x] [Scalable Architectures](../03-scalable-architectures/) - Event-driven patterns

**Technical Skills**:
- API design and development
- Event-driven programming concepts
- JSON data manipulation
- Basic understanding of NoSQL databases

## ‚è±Ô∏è Time Commitment

**Total Duration**: 4-5 weeks (25-30 hours)
- **Week 1**: Lambda fundamentals and API Gateway (7-8 hours)
- **Week 2**: Event-driven architectures (6-7 hours)
- **Week 3**: Data processing and Step Functions (6-7 hours)
- **Week 4**: Advanced patterns and optimization (3-4 hours)
- **Week 5**: Production deployment and monitoring (3-4 hours)

## üèóÔ∏è Serverless Architecture Patterns

### 1. Simple Web API Pattern
```
Client ‚Üí CloudFront ‚Üí API Gateway ‚Üí Lambda ‚Üí DynamoDB
                           ‚Üì
                       CloudWatch Logs
```

### 2. Event-Driven Processing Pattern
```
S3 Upload ‚Üí Lambda ‚Üí DynamoDB ‚Üí DynamoDB Streams ‚Üí Lambda ‚Üí SNS
                                        ‚Üì
                               ElasticSearch/OpenSearch
```

### 3. Data Pipeline Pattern
```
Kinesis ‚Üí Lambda ‚Üí Transform ‚Üí S3 ‚Üí Athena
   ‚Üì         ‚Üì                   ‚Üì
  SQS    CloudWatch         QuickSight
```

### 4. Microservices API Pattern
```
API Gateway
    ‚îú‚îÄ‚îÄ /users    ‚Üí Lambda (User Service)    ‚Üí RDS
    ‚îú‚îÄ‚îÄ /orders   ‚Üí Lambda (Order Service)   ‚Üí DynamoDB
    ‚îú‚îÄ‚îÄ /products ‚Üí Lambda (Product Service) ‚Üí ElastiCache
    ‚îî‚îÄ‚îÄ /payments ‚Üí Lambda (Payment Service) ‚Üí External API
```

## üóÇÔ∏è Section Contents

### Week 1: Lambda Fundamentals & API Gateway
- [Lambda Function Basics](lambda-functions/lambda-basics.md)
- [Function Configuration and Optimization](lambda-functions/optimization.md)
- [API Gateway REST APIs](api-gateway-lambda/rest-apis.md)
- [API Gateway WebSocket APIs](api-gateway-lambda/websocket-apis.md)

### Week 2: Event-Driven Systems
- [Event Sources and Triggers](event-driven-systems/event-sources.md)
- [SQS and SNS Integration](event-driven-systems/sqs-sns.md)
- [EventBridge Patterns](event-driven-systems/eventbridge.md)
- [DynamoDB Streams](event-driven-systems/dynamodb-streams.md)

### Week 3: Orchestration & Data Processing
- [Step Functions Workflows](step-functions/workflows.md)
- [Kinesis Data Processing](step-functions/kinesis-processing.md)
- [Batch Processing Patterns](step-functions/batch-processing.md)
- [Error Handling and Retries](step-functions/error-handling.md)

### Week 4: Advanced Patterns
- [CQRS with Serverless](advanced-patterns/cqrs-serverless.md)
- [Serverless GraphQL](advanced-patterns/graphql.md)
- [Multi-tenant Architecture](advanced-patterns/multi-tenant.md)
- [Cold Start Optimization](advanced-patterns/cold-start.md)

### Week 5: Production & Monitoring
- [Deployment Strategies](production/deployment.md)
- [Monitoring and Alerting](production/monitoring.md)
- [Cost Optimization](production/cost-optimization.md)
- [Security Best Practices](production/security.md)

## üõ†Ô∏è Hands-on Projects

### Project 1: Serverless Blog Platform (Week 1-2)
**Objective**: Build a complete blogging platform using serverless technologies

**Features**:
- User authentication with Cognito
- CRUD operations for blog posts
- File upload for images
- Search functionality
- Comments system

**Architecture**:
```
Frontend (S3/CloudFront) ‚Üí API Gateway ‚Üí Lambda Functions
                                            ‚Üì
                                        DynamoDB
                                            ‚Üì
                                     ElasticSearch
```

**Lambda Functions**:
- `createPost` - Create new blog post
- `getPosts` - Retrieve blog posts with pagination
- `updatePost` - Update existing post
- `deletePost` - Delete post
- `searchPosts` - Full-text search
- `uploadImage` - Handle image uploads to S3
- `addComment` - Add comment to post

### Project 2: Real-time Analytics Dashboard (Week 2-3)
**Objective**: Build real-time analytics processing and visualization

**Data Flow**:
```
Data Sources ‚Üí Kinesis ‚Üí Lambda ‚Üí DynamoDB ‚Üí API Gateway ‚Üí Dashboard
     ‚Üì                    ‚Üì           ‚Üì
   CloudWatch          S3        ElastiCache
```

**Components**:
- Real-time data ingestion with Kinesis
- Stream processing with Lambda
- Aggregated data storage in DynamoDB
- Caching layer with ElastiCache
- Real-time dashboard with WebSockets

### Project 3: E-commerce Order Processing (Week 3-4)
**Objective**: Implement complex order processing workflow with Step Functions

**Workflow Steps**:
1. Validate order
2. Check inventory
3. Process payment
4. Reserve inventory
5. Send confirmation
6. Schedule shipping

**Step Functions Definition**:
```json
{
  "Comment": "E-commerce order processing workflow",
  "StartAt": "ValidateOrder",
  "States": {
    "ValidateOrder": {
      "Type": "Task",
      "Resource": "arn:aws:lambda:region:account:function:ValidateOrder",
      "Next": "CheckInventory",
      "Catch": [
        {
          "ErrorEquals": ["ValidationError"],
          "Next": "OrderFailed"
        }
      ]
    },
    "CheckInventory": {
      "Type": "Task",
      "Resource": "arn:aws:lambda:region:account:function:CheckInventory",
      "Next": "ProcessPayment",
      "Catch": [
        {
          "ErrorEquals": ["InsufficientInventory"],
          "Next": "OrderFailed"
        }
      ]
    },
    "ProcessPayment": {
      "Type": "Task",
      "Resource": "arn:aws:lambda:region:account:function:ProcessPayment",
      "Next": "ReserveInventory",
      "Catch": [
        {
          "ErrorEquals": ["PaymentFailed"],
          "Next": "PaymentFailedHandler"
        }
      ]
    },
    "ReserveInventory": {
      "Type": "Task",
      "Resource": "arn:aws:lambda:region:account:function:ReserveInventory",
      "Next": "SendConfirmation"
    },
    "SendConfirmation": {
      "Type": "Task",
      "Resource": "arn:aws:lambda:region:account:function:SendConfirmation",
      "Next": "ScheduleShipping"
    },
    "ScheduleShipping": {
      "Type": "Task",
      "Resource": "arn:aws:lambda:region:account:function:ScheduleShipping",
      "End": true
    },
    "PaymentFailedHandler": {
      "Type": "Task",
      "Resource": "arn:aws:lambda:region:account:function:HandlePaymentFailure",
      "Next": "OrderFailed"
    },
    "OrderFailed": {
      "Type": "Fail",
      "Cause": "Order processing failed"
    }
  }
}
```

## üìã Lambda Function Patterns

### 1. HTTP API Handler Pattern
```python
import json
import boto3
from decimal import Decimal

def lambda_handler(event, context):
    # Parse HTTP event
    http_method = event['httpMethod']
    path = event['path']
    body = json.loads(event.get('body', '{}'))

    # Route handling
    if http_method == 'GET' and path == '/users':
        return get_users(event)
    elif http_method == 'POST' and path == '/users':
        return create_user(body)
    elif http_method == 'GET' and '/users/' in path:
        user_id = path.split('/')[-1]
        return get_user(user_id)
    else:
        return {
            'statusCode': 404,
            'headers': {
                'Content-Type': 'application/json',
                'Access-Control-Allow-Origin': '*'
            },
            'body': json.dumps({'error': 'Not found'})
        }

def get_users(event):
    dynamodb = boto3.resource('dynamodb')
    table = dynamodb.Table('Users')

    # Handle pagination
    query_params = event.get('queryStringParameters', {}) or {}
    limit = int(query_params.get('limit', 20))
    last_key = query_params.get('last_key')

    scan_kwargs = {'Limit': limit}
    if last_key:
        scan_kwargs['ExclusiveStartKey'] = {'id': last_key}

    response = table.scan(**scan_kwargs)

    result = {
        'users': decimal_to_float(response['Items']),
        'count': response['Count']
    }

    if 'LastEvaluatedKey' in response:
        result['last_key'] = response['LastEvaluatedKey']['id']

    return {
        'statusCode': 200,
        'headers': {
            'Content-Type': 'application/json',
            'Access-Control-Allow-Origin': '*'
        },
        'body': json.dumps(result)
    }

def decimal_to_float(obj):
    if isinstance(obj, list):
        return [decimal_to_float(i) for i in obj]
    elif isinstance(obj, dict):
        return {k: decimal_to_float(v) for k, v in obj.items()}
    elif isinstance(obj, Decimal):
        return float(obj)
    return obj
```

### 2. Event Processing Pattern
```python
import json
import boto3
from datetime import datetime

def lambda_handler(event, context):
    """Process events from various AWS services"""

    # Handle different event sources
    if 'Records' in event:
        if 'eventSource' in event['Records'][0]:
            source = event['Records'][0]['eventSource']

            if source == 'aws:s3':
                return process_s3_event(event)
            elif source == 'aws:dynamodb':
                return process_dynamodb_event(event)
            elif source == 'aws:sqs':
                return process_sqs_event(event)

    # Handle EventBridge events
    elif 'source' in event:
        return process_eventbridge_event(event)

    # Handle API Gateway events
    elif 'httpMethod' in event:
        return process_api_event(event)

    return {'statusCode': 400, 'body': 'Unsupported event type'}

def process_s3_event(event):
    """Process S3 bucket events"""
    s3_client = boto3.client('s3')

    for record in event['Records']:
        bucket = record['s3']['bucket']['name']
        key = record['s3']['object']['key']
        event_name = record['eventName']

        if event_name.startswith('ObjectCreated'):
            # Process new file upload
            process_file_upload(bucket, key)
        elif event_name.startswith('ObjectRemoved'):
            # Handle file deletion
            process_file_deletion(bucket, key)

    return {'statusCode': 200, 'body': 'S3 events processed'}

def process_dynamodb_event(event):
    """Process DynamoDB stream events"""
    for record in event['Records']:
        event_name = record['eventName']

        if event_name == 'INSERT':
            handle_new_record(record['dynamodb']['NewImage'])
        elif event_name == 'MODIFY':
            handle_record_update(
                record['dynamodb']['OldImage'],
                record['dynamodb']['NewImage']
            )
        elif event_name == 'REMOVE':
            handle_record_deletion(record['dynamodb']['OldImage'])

    return {'statusCode': 200, 'body': 'DynamoDB events processed'}

def process_file_upload(bucket, key):
    """Process uploaded file"""
    # Example: Generate thumbnail for images
    if key.lower().endswith(('.png', '.jpg', '.jpeg')):
        generate_thumbnail(bucket, key)

    # Example: Process CSV files
    elif key.lower().endswith('.csv'):
        process_csv_file(bucket, key)

    # Log the upload
    log_file_upload(bucket, key)

def generate_thumbnail(bucket, key):
    """Generate thumbnail for uploaded image"""
    # Implementation would use PIL/Pillow
    pass

def process_csv_file(bucket, key):
    """Process uploaded CSV file"""
    # Implementation would parse CSV and store in database
    pass
```

### 3. Data Transformation Pattern
```python
import json
import base64
import boto3
from datetime import datetime

def lambda_handler(event, context):
    """Transform data from Kinesis stream"""

    output = []

    for record in event['records']:
        # Decode the data
        payload = base64.b64decode(record['data'])
        data = json.loads(payload)

        # Transform the data
        transformed_data = transform_record(data)

        # Prepare output record
        output_record = {
            'recordId': record['recordId'],
            'result': 'Ok',
            'data': base64.b64encode(
                json.dumps(transformed_data).encode('utf-8')
            ).decode('utf-8')
        }
        output.append(output_record)

    return {'records': output}

def transform_record(data):
    """Transform individual record"""
    try:
        # Add timestamp
        data['processed_at'] = datetime.utcnow().isoformat()

        # Calculate derived fields
        if 'price' in data and 'quantity' in data:
            data['total_value'] = data['price'] * data['quantity']

        # Normalize data
        if 'email' in data:
            data['email'] = data['email'].lower().strip()

        # Add metadata
        data['processing_version'] = '1.0'

        return data

    except Exception as e:
        # Handle transformation errors
        return {
            'error': str(e),
            'original_data': data,
            'processed_at': datetime.utcnow().isoformat()
        }
```

## üîÑ Event-Driven Architecture Patterns

### 1. Event Sourcing with Lambda
```python
import json
import boto3
import uuid
from datetime import datetime

class EventStore:
    def __init__(self):
        self.dynamodb = boto3.resource('dynamodb')
        self.events_table = self.dynamodb.Table('EventStore')
        self.snapshots_table = self.dynamodb.Table('Snapshots')

    def append_events(self, aggregate_id, events, expected_version=-1):
        """Append events to the event store"""
        with self.events_table.batch_writer() as batch:
            for i, event in enumerate(events):
                event_record = {
                    'aggregateId': aggregate_id,
                    'eventId': str(uuid.uuid4()),
                    'eventType': event['type'],
                    'eventData': event['data'],
                    'version': expected_version + i + 1,
                    'timestamp': datetime.utcnow().isoformat()
                }
                batch.put_item(Item=event_record)

    def get_events(self, aggregate_id, from_version=0):
        """Get events for an aggregate from a specific version"""
        response = self.events_table.query(
            KeyConditionExpression=Key('aggregateId').eq(aggregate_id),
            FilterExpression=Attr('version').gt(from_version),
            ScanIndexForward=True
        )
        return response['Items']

def lambda_handler(event, context):
    """Handle command and generate events"""
    command = json.loads(event['body'])
    command_type = command['type']

    if command_type == 'CreateOrder':
        return handle_create_order(command)
    elif command_type == 'AddOrderItem':
        return handle_add_order_item(command)
    elif command_type == 'CancelOrder':
        return handle_cancel_order(command)

    return {'statusCode': 400, 'body': 'Unknown command type'}

def handle_create_order(command):
    """Handle create order command"""
    order_id = str(uuid.uuid4())

    events = [
        {
            'type': 'OrderCreated',
            'data': {
                'orderId': order_id,
                'customerId': command['customerId'],
                'createdAt': datetime.utcnow().isoformat()
            }
        }
    ]

    # Store events
    event_store = EventStore()
    event_store.append_events(order_id, events)

    # Publish events to EventBridge
    publish_events(events)

    return {
        'statusCode': 201,
        'body': json.dumps({'orderId': order_id})
    }

def publish_events(events):
    """Publish events to EventBridge"""
    eventbridge = boto3.client('events')

    entries = []
    for event in events:
        entry = {
            'Source': 'order-service',
            'DetailType': event['type'],
            'Detail': json.dumps(event['data']),
            'EventBusName': 'order-events'
        }
        entries.append(entry)

    eventbridge.put_events(Entries=entries)
```

### 2. Saga Pattern with Step Functions
```python
import json
import boto3

def lambda_handler(event, context):
    """Saga coordinator using Step Functions"""

    step_functions = boto3.client('stepfunctions')

    # Start saga workflow
    if event.get('action') == 'start_saga':
        return start_saga(event, step_functions)

    # Handle saga step completion
    elif event.get('action') == 'complete_step':
        return complete_saga_step(event, step_functions)

    # Handle saga compensation
    elif event.get('action') == 'compensate':
        return compensate_saga(event, step_functions)

def start_saga(event, step_functions):
    """Start a new saga workflow"""
    saga_data = event['sagaData']

    execution = step_functions.start_execution(
        stateMachineArn=os.environ['SAGA_STATE_MACHINE_ARN'],
        input=json.dumps(saga_data)
    )

    return {
        'statusCode': 200,
        'body': json.dumps({
            'sagaId': execution['executionArn'].split(':')[-1],
            'executionArn': execution['executionArn']
        })
    }

# Individual saga step functions
def reserve_inventory(event, context):
    """Reserve inventory for order"""
    try:
        order_data = event['orderData']

        # Call inventory service
        inventory_client = InventoryServiceClient()
        reservation = inventory_client.reserve_items(order_data['items'])

        return {
            'status': 'SUCCESS',
            'reservationId': reservation['id'],
            'orderData': order_data
        }
    except InsufficientInventoryError as e:
        return {
            'status': 'FAILED',
            'error': str(e),
            'orderData': order_data
        }

def process_payment(event, context):
    """Process payment for order"""
    try:
        order_data = event['orderData']

        # Call payment service
        payment_client = PaymentServiceClient()
        payment = payment_client.charge(
            order_data['customerId'],
            order_data['amount']
        )

        return {
            'status': 'SUCCESS',
            'paymentId': payment['id'],
            'orderData': order_data
        }
    except PaymentFailedError as e:
        return {
            'status': 'FAILED',
            'error': str(e),
            'orderData': order_data
        }

def compensate_inventory(event, context):
    """Compensate inventory reservation"""
    reservation_id = event['reservationId']

    inventory_client = InventoryServiceClient()
    inventory_client.release_reservation(reservation_id)

    return {
        'status': 'COMPENSATED',
        'reservationId': reservation_id
    }
```

## üí∞ Cost Optimization Strategies

### 1. Lambda Optimization
```python
# Memory and timeout optimization
{
    "FunctionName": "order-processor",
    "Runtime": "python3.9",
    "MemorySize": 512,  # Optimized based on monitoring
    "Timeout": 30,      # Minimum required
    "Environment": {
        "Variables": {
            "DB_CONNECTION_POOL_SIZE": "5"
        }
    },
    "DeadLetterConfig": {
        "TargetArn": "arn:aws:sqs:region:account:dlq"
    }
}

# Connection reuse
import boto3

# Initialize outside handler for connection reuse
dynamodb = boto3.resource('dynamodb')
table = dynamodb.Table('Orders')

def lambda_handler(event, context):
    # Reuse connection across invocations
    response = table.get_item(Key={'id': event['orderId']})
    return response
```

### 2. Provisioned Concurrency
```yaml
# CloudFormation template for provisioned concurrency
OrderProcessorFunction:
  Type: AWS::Lambda::Function
  Properties:
    FunctionName: OrderProcessor
    ProvisionedConcurrencyConfig:
      ProvisionedConcurrencyVersions:
        - Version: !GetAtt OrderProcessorVersion.Version
          ProvisionedConcurrency: 10

OrderProcessorAlias:
  Type: AWS::Lambda::Alias
  Properties:
    FunctionName: !Ref OrderProcessorFunction
    FunctionVersion: !GetAtt OrderProcessorVersion.Version
    Name: LIVE
    ProvisionedConcurrencyConfig:
      ProvisionedConcurrency: 10
```

### 3. Cost Monitoring
```python
def lambda_handler(event, context):
    """Lambda function with cost tracking"""

    start_time = time.time()

    try:
        # Main function logic
        result = process_order(event)

        # Track metrics
        track_invocation_metrics(
            function_name=context.function_name,
            duration=time.time() - start_time,
            memory_used=context.memory_limit_in_mb,
            status='success'
        )

        return result

    except Exception as e:
        track_invocation_metrics(
            function_name=context.function_name,
            duration=time.time() - start_time,
            memory_used=context.memory_limit_in_mb,
            status='error'
        )
        raise

def track_invocation_metrics(function_name, duration, memory_used, status):
    """Track custom metrics for cost analysis"""
    cloudwatch = boto3.client('cloudwatch')

    # Calculate estimated cost
    gb_seconds = (memory_used / 1024) * duration
    estimated_cost = gb_seconds * 0.0000166667  # Current Lambda pricing

    cloudwatch.put_metric_data(
        Namespace='Lambda/CostTracking',
        MetricData=[
            {
                'MetricName': 'EstimatedCost',
                'Value': estimated_cost,
                'Unit': 'None',
                'Dimensions': [
                    {'Name': 'FunctionName', 'Value': function_name},
                    {'Name': 'Status', 'Value': status}
                ]
            },
            {
                'MetricName': 'Duration',
                'Value': duration * 1000,  # Convert to milliseconds
                'Unit': 'Milliseconds',
                'Dimensions': [
                    {'Name': 'FunctionName', 'Value': function_name}
                ]
            }
        ]
    )
```

## üìä Performance Monitoring

### Custom Metrics and Alarms
```python
import boto3
import time
from functools import wraps

def monitor_performance(metric_name):
    """Decorator to monitor Lambda performance"""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            start_time = time.time()

            try:
                result = func(*args, **kwargs)
                status = 'success'
                return result
            except Exception as e:
                status = 'error'
                raise
            finally:
                duration = time.time() - start_time

                # Send custom metrics to CloudWatch
                cloudwatch = boto3.client('cloudwatch')
                cloudwatch.put_metric_data(
                    Namespace='Lambda/Performance',
                    MetricData=[
                        {
                            'MetricName': f'{metric_name}_Duration',
                            'Value': duration * 1000,
                            'Unit': 'Milliseconds',
                            'Dimensions': [
                                {'Name': 'Status', 'Value': status}
                            ]
                        }
                    ]
                )
        return wrapper
    return decorator

@monitor_performance('OrderProcessing')
def lambda_handler(event, context):
    """Main Lambda handler with monitoring"""
    return process_order(event)
```

## üîí Security Best Practices

### 1. IAM Least Privilege
```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "dynamodb:GetItem",
        "dynamodb:PutItem",
        "dynamodb:UpdateItem"
      ],
      "Resource": "arn:aws:dynamodb:region:account:table/Orders"
    },
    {
      "Effect": "Allow",
      "Action": [
        "sqs:SendMessage"
      ],
      "Resource": "arn:aws:sqs:region:account:order-processing-queue"
    },
    {
      "Effect": "Allow",
      "Action": [
        "logs:CreateLogGroup",
        "logs:CreateLogStream",
        "logs:PutLogEvents"
      ],
      "Resource": "arn:aws:logs:region:account:*"
    }
  ]
}
```

### 2. Environment Variables and Secrets
```python
import os
import boto3
import json

def get_secret(secret_name):
    """Retrieve secret from AWS Secrets Manager"""
    secrets_client = boto3.client('secretsmanager')

    try:
        response = secrets_client.get_secret_value(SecretId=secret_name)
        return json.loads(response['SecretString'])
    except Exception as e:
        print(f"Error retrieving secret {secret_name}: {e}")
        raise

def lambda_handler(event, context):
    # Use environment variables for non-sensitive config
    table_name = os.environ['ORDERS_TABLE']
    queue_url = os.environ['PROCESSING_QUEUE_URL']

    # Use Secrets Manager for sensitive data
    db_credentials = get_secret('rds-credentials')
    api_key = get_secret('payment-api-key')

    # Process with secure configuration
    return process_order(event, table_name, queue_url, db_credentials)
```

## üéØ Success Criteria

### Technical Achievements
- [ ] **Performance**: < 200ms average response time for APIs
- [ ] **Scalability**: Handle 1000+ concurrent executions
- [ ] **Reliability**: 99.9% success rate for function executions
- [ ] **Cost Efficiency**: 50% cost reduction compared to container solutions
- [ ] **Security**: Zero security vulnerabilities in audit

### Operational Excellence
- [ ] **Monitoring**: Comprehensive observability and alerting
- [ ] **Deployment**: Automated CI/CD pipelines
- [ ] **Error Handling**: Graceful degradation and retry mechanisms
- [ ] **Documentation**: Complete API documentation and runbooks
- [ ] **Testing**: Unit, integration, and load tests

## üöÄ Next Steps

After mastering serverless architecture:
1. **Explore [Data Engineering](../06-data-engineering/)** - Serverless data pipelines
2. **Implement [Monitoring](../09-monitoring-observability/)** - Advanced observability
3. **Build [Real-world Projects](../07-real-world-projects/)** - Apply serverless patterns

---

**Ready to go serverless?** Start with [Lambda Functions](lambda-functions/) to begin your serverless journey!

*Remember: Serverless doesn't mean "no servers"‚Äîit means no server management. Focus on your business logic, not infrastructure.*