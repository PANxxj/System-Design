# High Level System Design (HLSD) - Advanced Concepts

## Table of Contents
1. [Global Scale Architecture](#global-scale-architecture)
2. [Advanced Distributed Algorithms](#advanced-distributed-algorithms)
3. [Multi-Region and Multi-Cloud Systems](#multi-region-and-multi-cloud-systems)
4. [Event-Driven Architecture at Scale](#event-driven-architecture-at-scale)
5. [Advanced Data Management](#advanced-data-management)
6. [Performance Optimization](#performance-optimization)
7. [Chaos Engineering and Resilience](#chaos-engineering-and-resilience)
8. [Security at Scale](#security-at-scale)
9. [Machine Learning Systems Architecture](#machine-learning-systems-architecture)
10. [Real-time Systems and Stream Processing](#real-time-systems-and-stream-processing)
11. [Complex System Design Examples](#complex-system-design-examples)

## Global Scale Architecture

### Geographic Distribution Strategies

#### 1. Content Delivery Networks (CDN) Design

```python
class GlobalCDN:
    def __init__(self):
        self.edge_locations = {}  # region -> EdgeLocation
        self.origin_servers = {}  # region -> OriginServer
        self.routing_table = {}   # content_id -> optimal_edge_location
        self.cache_invalidation_queue = queue.Queue()

    def setup_edge_location(self, region, capacity, cache_policy):
        """Setup edge location in a region"""
        self.edge_locations[region] = EdgeLocation(
            region=region,
            capacity=capacity,
            cache_policy=cache_policy,
            parent_cdn=self
        )

    def route_request(self, user_location, content_id):
        """Route user request to optimal edge location"""
        # 1. Determine closest edge locations
        closest_edges = self._find_closest_edges(user_location, limit=3)

        # 2. Check cache hit and capacity
        for edge in closest_edges:
            if edge.has_content(content_id) and edge.has_capacity():
                return edge.serve_content(content_id)

        # 3. Cache miss - fetch from origin
        optimal_edge = closest_edges[0]
        content = self._fetch_from_origin(content_id, optimal_edge.region)
        optimal_edge.cache_content(content_id, content)

        return optimal_edge.serve_content(content_id)

    def _find_closest_edges(self, user_location, limit=3):
        """Find closest edge locations using geolocation"""
        edges_with_distance = []
        for region, edge in self.edge_locations.items():
            distance = self._calculate_geographic_distance(
                user_location, edge.location
            )
            edges_with_distance.append((distance, edge))

        # Sort by distance and return top N
        edges_with_distance.sort(key=lambda x: x[0])
        return [edge for _, edge in edges_with_distance[:limit]]

class EdgeLocation:
    def __init__(self, region, capacity, cache_policy, parent_cdn):
        self.region = region
        self.capacity = capacity
        self.cache_policy = cache_policy
        self.parent_cdn = parent_cdn
        self.cache = LRUCache(capacity)
        self.request_count = 0
        self.bandwidth_usage = 0

    def serve_content(self, content_id):
        """Serve content and update metrics"""
        self.request_count += 1
        content = self.cache.get(content_id)
        if content:
            self.bandwidth_usage += len(content)
            # Update cache access for LRU
            return content
        return None

    def cache_content(self, content_id, content):
        """Cache content with eviction policy"""
        if self.cache_policy.should_cache(content_id, content):
            self.cache.put(content_id, content)

            # Prefetch related content if applicable
            related_content = self._get_related_content(content_id)
            for related_id in related_content:
                if self.cache.has_space():
                    self._prefetch_content(related_id)
```

#### 2. Multi-Region Database Replication

```python
class GlobalDatabaseCluster:
    def __init__(self):
        self.regions = {}  # region -> DatabaseRegion
        self.master_region = None
        self.replication_lag_monitor = ReplicationLagMonitor()
        self.conflict_resolver = ConflictResolver()

    def setup_region(self, region_name, is_master=False):
        """Setup database region"""
        db_region = DatabaseRegion(
            region_name=region_name,
            is_master=is_master,
            cluster=self
        )
        self.regions[region_name] = db_region

        if is_master:
            self.master_region = region_name

    def write_operation(self, operation, region=None):
        """Handle write operation with global coordination"""
        target_region = region or self.master_region

        if target_region == self.master_region:
            # Master write - replicate to all regions
            result = self.regions[self.master_region].execute_write(operation)
            self._replicate_to_all_regions(operation, exclude=self.master_region)
            return result
        else:
            # Regional write - needs conflict resolution
            return self._handle_regional_write(operation, target_region)

    def read_operation(self, query, consistency_level='eventual'):
        """Handle read operation with consistency guarantees"""
        if consistency_level == 'strong':
            # Read from master only
            return self.regions[self.master_region].execute_read(query)

        elif consistency_level == 'bounded_staleness':
            # Read from region with acceptable lag
            acceptable_lag_ms = 1000  # 1 second
            for region_name, region in self.regions.items():
                lag = self.replication_lag_monitor.get_lag(region_name)
                if lag <= acceptable_lag_ms:
                    return region.execute_read(query)

            # Fallback to master if all regions are too stale
            return self.regions[self.master_region].execute_read(query)

        else:  # eventual consistency
            # Read from any available region
            return self._read_from_available_region(query)

    def _handle_regional_write(self, operation, region):
        """Handle write operation in non-master region"""
        # Option 1: Forward to master
        if self._should_forward_to_master(operation):
            return self.write_operation(operation, self.master_region)

        # Option 2: Accept locally and resolve conflicts later
        local_result = self.regions[region].execute_write(operation)
        self._schedule_conflict_resolution(operation, region)
        return local_result

class ConflictResolver:
    def __init__(self):
        self.resolution_strategies = {
            'last_write_wins': self._last_write_wins,
            'application_defined': self._application_defined,
            'vector_clock': self._vector_clock_resolution
        }

    def resolve_conflict(self, conflicting_operations, strategy='last_write_wins'):
        """Resolve conflicts between operations"""
        resolver = self.resolution_strategies.get(strategy)
        if not resolver:
            raise ValueError(f"Unknown resolution strategy: {strategy}")

        return resolver(conflicting_operations)

    def _last_write_wins(self, operations):
        """Resolve using timestamp-based last-write-wins"""
        return max(operations, key=lambda op: op.timestamp)

    def _vector_clock_resolution(self, operations):
        """Resolve using vector clock comparison"""
        # Compare vector clocks to determine causal ordering
        for op1 in operations:
            is_concurrent = True
            for op2 in operations:
                if op1 != op2:
                    if self._vector_clock_precedes(op1.vector_clock, op2.vector_clock):
                        is_concurrent = False
                        break

            if is_concurrent:
                # Use timestamp as tie-breaker for concurrent operations
                return max(operations, key=lambda op: op.timestamp)
```

### Global Load Balancing

```python
class GlobalLoadBalancer:
    def __init__(self):
        self.regional_clusters = {}  # region -> ClusterInfo
        self.health_monitor = GlobalHealthMonitor()
        self.traffic_monitor = TrafficMonitor()
        self.cost_optimizer = CostOptimizer()

    def route_request(self, request):
        """Route request to optimal regional cluster"""
        user_location = self._get_user_location(request)

        # Get candidate regions with health and capacity info
        candidates = self._get_candidate_regions(user_location)

        # Apply routing algorithm
        optimal_region = self._select_optimal_region(
            candidates, request, user_location
        )

        return self._forward_to_region(request, optimal_region)

    def _select_optimal_region(self, candidates, request, user_location):
        """Select optimal region using multiple factors"""
        scored_candidates = []

        for region in candidates:
            score = self._calculate_region_score(region, request, user_location)
            scored_candidates.append((score, region))

        # Sort by score (higher is better)
        scored_candidates.sort(key=lambda x: x[0], reverse=True)
        return scored_candidates[0][1]

    def _calculate_region_score(self, region, request, user_location):
        """Calculate region score based on multiple factors"""
        factors = {
            'latency': self._calculate_latency_score(region, user_location),
            'capacity': self._calculate_capacity_score(region),
            'cost': self._calculate_cost_score(region),
            'availability': self._calculate_availability_score(region),
            'compliance': self._calculate_compliance_score(region, request)
        }

        # Weighted scoring
        weights = {
            'latency': 0.4,
            'capacity': 0.25,
            'cost': 0.15,
            'availability': 0.15,
            'compliance': 0.05
        }

        total_score = sum(
            factors[factor] * weights[factor]
            for factor in factors
        )

        return total_score

class CostOptimizer:
    def __init__(self):
        self.pricing_model = PricingModel()
        self.usage_patterns = UsagePatternAnalyzer()

    def optimize_regional_distribution(self, traffic_patterns):
        """Optimize traffic distribution for cost efficiency"""
        optimization_strategy = {
            'peak_hours': self._peak_hour_strategy(traffic_patterns),
            'off_peak': self._off_peak_strategy(traffic_patterns),
            'weekend': self._weekend_strategy(traffic_patterns)
        }

        return optimization_strategy

    def _peak_hour_strategy(self, traffic_patterns):
        """Strategy for peak traffic hours"""
        return {
            'prioritize': ['capacity', 'latency'],
            'cost_weight': 0.1,  # Lower cost priority during peak
            'auto_scale': True,
            'use_spot_instances': False
        }

    def _off_peak_strategy(self, traffic_patterns):
        """Strategy for off-peak hours"""
        return {
            'prioritize': ['cost', 'availability'],
            'cost_weight': 0.4,  # Higher cost priority off-peak
            'auto_scale': True,
            'use_spot_instances': True
        }
```

## Advanced Distributed Algorithms

### Byzantine Fault Tolerance

```python
class ByzantineFaultTolerantConsensus:
    """Implementation of PBFT (Practical Byzantine Fault Tolerance)"""

    def __init__(self, node_id, total_nodes):
        self.node_id = node_id
        self.total_nodes = total_nodes
        self.fault_tolerance = (total_nodes - 1) // 3  # Max f = (n-1)/3
        self.view_number = 0
        self.sequence_number = 0
        self.state = "NORMAL"  # NORMAL, VIEW_CHANGE

        # Message logs
        self.pre_prepare_log = {}
        self.prepare_log = defaultdict(set)
        self.commit_log = defaultdict(set)
        self.executed_requests = set()

    def propose_request(self, request):
        """Primary node proposes a request"""
        if not self._is_primary():
            raise Exception("Only primary can propose requests")

        self.sequence_number += 1
        pre_prepare_msg = {
            'type': 'PRE_PREPARE',
            'view': self.view_number,
            'sequence': self.sequence_number,
            'request': request,
            'digest': self._compute_digest(request)
        }

        # Log pre-prepare message
        self.pre_prepare_log[self.sequence_number] = pre_prepare_msg

        # Broadcast to all backup nodes
        self._broadcast_to_backups(pre_prepare_msg)

    def handle_pre_prepare(self, pre_prepare_msg):
        """Handle PRE_PREPARE message from primary"""
        if not self._validate_pre_prepare(pre_prepare_msg):
            return

        # Log the pre-prepare
        seq_num = pre_prepare_msg['sequence']
        self.pre_prepare_log[seq_num] = pre_prepare_msg

        # Send PREPARE message
        prepare_msg = {
            'type': 'PREPARE',
            'view': self.view_number,
            'sequence': seq_num,
            'digest': pre_prepare_msg['digest'],
            'node_id': self.node_id
        }

        self._broadcast_to_all(prepare_msg)

    def handle_prepare(self, prepare_msg):
        """Handle PREPARE message from backup node"""
        if not self._validate_prepare(prepare_msg):
            return

        seq_num = prepare_msg['sequence']
        self.prepare_log[seq_num].add(prepare_msg['node_id'])

        # Check if we have enough PREPARE messages (2f)
        if len(self.prepare_log[seq_num]) >= 2 * self.fault_tolerance:
            # Send COMMIT message
            commit_msg = {
                'type': 'COMMIT',
                'view': self.view_number,
                'sequence': seq_num,
                'digest': prepare_msg['digest'],
                'node_id': self.node_id
            }

            self._broadcast_to_all(commit_msg)

    def handle_commit(self, commit_msg):
        """Handle COMMIT message"""
        if not self._validate_commit(commit_msg):
            return

        seq_num = commit_msg['sequence']
        self.commit_log[seq_num].add(commit_msg['node_id'])

        # Check if we have enough COMMIT messages (2f + 1)
        if len(self.commit_log[seq_num]) >= 2 * self.fault_tolerance + 1:
            # Execute the request
            if seq_num in self.pre_prepare_log:
                request = self.pre_prepare_log[seq_num]['request']
                self._execute_request(request)
                self.executed_requests.add(seq_num)

    def _is_primary(self):
        """Check if this node is the primary for current view"""
        return (self.view_number % self.total_nodes) == self.node_id

    def _validate_pre_prepare(self, msg):
        """Validate PRE_PREPARE message"""
        # Check view number, sequence number, digest, etc.
        return (
            msg['view'] == self.view_number and
            msg['sequence'] > max(self.executed_requests, default=0) and
            self._verify_digest(msg['request'], msg['digest'])
        )

    def initiate_view_change(self):
        """Initiate view change when primary is suspected faulty"""
        self.view_number += 1
        self.state = "VIEW_CHANGE"

        view_change_msg = {
            'type': 'VIEW_CHANGE',
            'new_view': self.view_number,
            'last_sequence': self.sequence_number,
            'node_id': self.node_id,
            'prepared_requests': self._get_prepared_requests()
        }

        self._broadcast_to_all(view_change_msg)

class DistributedLocking:
    """Distributed locking using Redlock algorithm"""

    def __init__(self, redis_instances):
        self.redis_instances = redis_instances
        self.quorum = len(redis_instances) // 2 + 1
        self.clock_drift_factor = 0.01
        self.retry_delay = 0.2
        self.retry_count = 3

    def acquire_lock(self, resource, ttl_ms=30000):
        """Acquire distributed lock using Redlock algorithm"""
        for attempt in range(self.retry_count):
            start_time = self._current_time_ms()

            # Try to acquire lock on majority of instances
            acquired_instances = []
            for instance in self.redis_instances:
                if self._acquire_single_lock(instance, resource, ttl_ms):
                    acquired_instances.append(instance)

            # Calculate drift time
            drift = (ttl_ms * self.clock_drift_factor) + 2

            # Check if we acquired majority and still have valid time
            elapsed_time = self._current_time_ms() - start_time
            validity_time = ttl_ms - elapsed_time - drift

            if len(acquired_instances) >= self.quorum and validity_time > 0:
                return LockContext(
                    resource=resource,
                    instances=acquired_instances,
                    validity_time=validity_time,
                    locker=self
                )
            else:
                # Failed to acquire lock, release any acquired locks
                self._release_locks(acquired_instances, resource)
                time.sleep(self.retry_delay)

        return None  # Failed to acquire lock

    def _acquire_single_lock(self, instance, resource, ttl_ms):
        """Acquire lock on single Redis instance"""
        try:
            # Use SET with NX (only if not exists) and PX (TTL in ms)
            result = instance.set(
                f"lock:{resource}",
                self._generate_lock_value(),
                nx=True,
                px=ttl_ms
            )
            return result is True
        except:
            return False

class LockContext:
    """Context manager for distributed lock"""

    def __init__(self, resource, instances, validity_time, locker):
        self.resource = resource
        self.instances = instances
        self.validity_time = validity_time
        self.locker = locker

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.locker._release_locks(self.instances, self.resource)
```

### Vector Clocks and Causal Ordering

```python
class VectorClock:
    """Vector clock implementation for causal ordering"""

    def __init__(self, node_id, initial_clocks=None):
        self.node_id = node_id
        self.clocks = initial_clocks or {node_id: 0}

    def tick(self):
        """Increment local clock"""
        self.clocks[self.node_id] = self.clocks.get(self.node_id, 0) + 1

    def update(self, other_clock):
        """Update vector clock when receiving message"""
        # Take maximum of each component
        for node_id, clock_value in other_clock.clocks.items():
            self.clocks[node_id] = max(
                self.clocks.get(node_id, 0),
                clock_value
            )

        # Increment local clock
        self.tick()

    def happens_before(self, other):
        """Check if this event happens before other"""
        # self < other if: self[i] <= other[i] for all i, and exists j: self[j] < other[j]
        all_nodes = set(self.clocks.keys()) | set(other.clocks.keys())

        strictly_less = False
        for node in all_nodes:
            self_clock = self.clocks.get(node, 0)
            other_clock = other.clocks.get(node, 0)

            if self_clock > other_clock:
                return False
            elif self_clock < other_clock:
                strictly_less = True

        return strictly_less

    def concurrent_with(self, other):
        """Check if events are concurrent (no causal relationship)"""
        return not self.happens_before(other) and not other.happens_before(self)

class CausalMessageOrdering:
    """Ensures causal ordering of messages"""

    def __init__(self, node_id):
        self.node_id = node_id
        self.vector_clock = VectorClock(node_id)
        self.message_buffer = []  # Buffer for out-of-order messages
        self.delivered_messages = set()

    def send_message(self, recipient, content):
        """Send message with vector clock"""
        self.vector_clock.tick()

        message = {
            'sender': self.node_id,
            'recipient': recipient,
            'content': content,
            'vector_clock': VectorClock(self.node_id, self.vector_clock.clocks.copy()),
            'message_id': self._generate_message_id()
        }

        return message

    def receive_message(self, message):
        """Receive message and ensure causal ordering"""
        if message['message_id'] in self.delivered_messages:
            return  # Already delivered

        # Check if message can be delivered now
        if self._can_deliver(message):
            self._deliver_message(message)
            self._try_deliver_buffered_messages()
        else:
            # Buffer message for later delivery
            self.message_buffer.append(message)

    def _can_deliver(self, message):
        """Check if message satisfies causal delivery condition"""
        msg_clock = message['vector_clock']
        sender = message['sender']

        # Condition 1: VC_sender[sender] = local_VC[sender] + 1
        if msg_clock.clocks.get(sender, 0) != self.vector_clock.clocks.get(sender, 0) + 1:
            return False

        # Condition 2: VC_sender[k] <= local_VC[k] for all k != sender
        for node, clock_value in msg_clock.clocks.items():
            if node != sender:
                if clock_value > self.vector_clock.clocks.get(node, 0):
                    return False

        return True

    def _deliver_message(self, message):
        """Deliver message and update vector clock"""
        # Update vector clock
        self.vector_clock.update(message['vector_clock'])

        # Mark as delivered
        self.delivered_messages.add(message['message_id'])

        # Process message content
        self._process_message_content(message['content'])

    def _try_deliver_buffered_messages(self):
        """Try to deliver buffered messages"""
        delivered_any = True
        while delivered_any:
            delivered_any = False
            for i, message in enumerate(self.message_buffer):
                if self._can_deliver(message):
                    self._deliver_message(message)
                    self.message_buffer.pop(i)
                    delivered_any = True
                    break
```

## Multi-Region and Multi-Cloud Systems

### Cross-Region Data Synchronization

```python
class CrossRegionSyncManager:
    def __init__(self):
        self.regions = {}  # region_id -> RegionController
        self.sync_policies = {}  # data_type -> SyncPolicy
        self.conflict_resolver = MultiRegionConflictResolver()
        self.bandwidth_manager = BandwidthManager()

    def setup_region(self, region_id, region_config):
        """Setup a new region for synchronization"""
        self.regions[region_id] = RegionController(
            region_id=region_id,
            config=region_config,
            sync_manager=self
        )

    def configure_sync_policy(self, data_type, policy):
        """Configure synchronization policy for data type"""
        self.sync_policies[data_type] = policy

    def sync_data_change(self, source_region, data_type, change_event):
        """Synchronize data change across regions"""
        policy = self.sync_policies.get(data_type)
        if not policy:
            return

        # Determine target regions based on policy
        target_regions = self._get_target_regions(policy, source_region, change_event)

        # Apply bandwidth throttling
        sync_tasks = []
        for target_region in target_regions:
            if self.bandwidth_manager.can_sync(source_region, target_region):
                task = SyncTask(
                    source=source_region,
                    target=target_region,
                    data_type=data_type,
                    change_event=change_event,
                    priority=policy.priority
                )
                sync_tasks.append(task)

        # Execute sync tasks with proper ordering
        self._execute_sync_tasks(sync_tasks)

    def _execute_sync_tasks(self, tasks):
        """Execute sync tasks with conflict resolution"""
        # Group by target region to detect conflicts
        region_tasks = defaultdict(list)
        for task in tasks:
            region_tasks[task.target].append(task)

        for target_region, region_task_list in region_tasks.items():
            if len(region_task_list) > 1:
                # Multiple changes for same region - resolve conflicts
                resolved_tasks = self.conflict_resolver.resolve_conflicts(region_task_list)
                for task in resolved_tasks:
                    self._execute_single_sync(task)
            else:
                self._execute_single_sync(region_task_list[0])

class MultiCloudOrchestrator:
    """Orchestrates operations across multiple cloud providers"""

    def __init__(self):
        self.cloud_providers = {}  # provider_name -> CloudProvider
        self.cost_optimizer = MultiCloudCostOptimizer()
        self.failover_manager = FailoverManager()
        self.compliance_manager = ComplianceManager()

    def deploy_workload(self, workload_spec, constraints=None):
        """Deploy workload across multiple cloud providers"""
        # Analyze constraints
        deployment_constraints = self._analyze_constraints(constraints)

        # Get candidate providers
        candidates = self._get_candidate_providers(workload_spec, deployment_constraints)

        # Optimize deployment strategy
        deployment_plan = self.cost_optimizer.optimize_deployment(
            workload_spec, candidates, deployment_constraints
        )

        # Execute deployment
        deployment_results = {}
        for provider_name, deployment_config in deployment_plan.items():
            try:
                result = self.cloud_providers[provider_name].deploy(
                    workload_spec, deployment_config
                )
                deployment_results[provider_name] = result
            except Exception as e:
                # Handle deployment failure
                self._handle_deployment_failure(provider_name, e, deployment_plan)

        return deployment_results

    def _analyze_constraints(self, constraints):
        """Analyze deployment constraints"""
        return {
            'latency_requirements': constraints.get('max_latency', float('inf')),
            'cost_budget': constraints.get('max_cost', float('inf')),
            'compliance_regions': constraints.get('allowed_regions', []),
            'availability_requirements': constraints.get('min_availability', 0.99),
            'data_sovereignty': constraints.get('data_sovereignty', {}),
            'disaster_recovery': constraints.get('dr_requirements', {})
        }

class MultiCloudCostOptimizer:
    def __init__(self):
        self.pricing_models = {}  # provider -> PricingModel
        self.usage_predictors = {}  # provider -> UsagePredictor

    def optimize_deployment(self, workload_spec, candidates, constraints):
        """Optimize deployment across cloud providers for cost"""
        optimization_models = []

        for provider in candidates:
            model = self._build_cost_model(provider, workload_spec, constraints)
            optimization_models.append(model)

        # Use linear programming or heuristic optimization
        optimal_allocation = self._solve_optimization_problem(
            optimization_models, constraints
        )

        return optimal_allocation

    def _build_cost_model(self, provider, workload_spec, constraints):
        """Build cost model for provider"""
        pricing = self.pricing_models[provider.name]

        # Compute costs for different resource types
        compute_cost = pricing.compute_cost(
            workload_spec.cpu_requirements,
            workload_spec.memory_requirements,
            workload_spec.duration
        )

        storage_cost = pricing.storage_cost(
            workload_spec.storage_requirements,
            workload_spec.storage_type
        )

        network_cost = pricing.network_cost(
            workload_spec.bandwidth_requirements,
            workload_spec.data_transfer
        )

        # Factor in reserved instances, spot pricing
        total_cost = self._calculate_optimized_cost(
            compute_cost, storage_cost, network_cost, provider
        )

        return CostModel(
            provider=provider,
            total_cost=total_cost,
            performance_score=self._calculate_performance_score(provider, workload_spec),
            compliance_score=self._calculate_compliance_score(provider, constraints)
        )
```

### Multi-Cloud Data Management

```python
class MultiCloudDataManager:
    def __init__(self):
        self.data_stores = {}  # store_id -> DataStore
        self.replication_policies = {}  # data_type -> ReplicationPolicy
        self.consistency_manager = CrossCloudConsistencyManager()
        self.cost_optimizer = DataStorageCostOptimizer()

    def store_data(self, data_id, data, metadata):
        """Store data across multiple cloud providers"""
        data_type = metadata.get('type')
        policy = self.replication_policies.get(data_type)

        if not policy:
            raise ValueError(f"No replication policy for data type: {data_type}")

        # Determine storage locations
        storage_locations = self._select_storage_locations(data, metadata, policy)

        # Store data with appropriate consistency level
        storage_results = {}
        for location in storage_locations:
            try:
                result = self._store_in_location(location, data_id, data, metadata)
                storage_results[location.store_id] = result
            except Exception as e:
                self._handle_storage_failure(location, data_id, e)

        # Update metadata with storage locations
        self._update_data_metadata(data_id, storage_results)

        return storage_results

    def retrieve_data(self, data_id, consistency_level='eventual'):
        """Retrieve data with specified consistency level"""
        metadata = self._get_data_metadata(data_id)
        storage_locations = metadata['storage_locations']

        if consistency_level == 'strong':
            # Read from primary store
            primary_store = self._get_primary_store(storage_locations)
            return primary_store.read(data_id)

        elif consistency_level == 'bounded_staleness':
            # Read from store with acceptable staleness
            max_staleness = metadata.get('max_staleness', 30000)  # 30 seconds
            for store_id in storage_locations:
                store = self.data_stores[store_id]
                if store.get_staleness(data_id) <= max_staleness:
                    return store.read(data_id)

        else:  # eventual consistency
            # Read from closest/fastest available store
            return self._read_from_optimal_store(data_id, storage_locations)

    def _select_storage_locations(self, data, metadata, policy):
        """Select optimal storage locations based on policy"""
        candidate_stores = self._get_candidate_stores(metadata)

        # Apply policy constraints
        filtered_stores = []
        for store in candidate_stores:
            if self._meets_policy_constraints(store, policy, metadata):
                filtered_stores.append(store)

        # Optimize based on cost, latency, compliance
        optimization_criteria = {
            'cost_weight': policy.cost_weight,
            'latency_weight': policy.latency_weight,
            'compliance_weight': policy.compliance_weight,
            'durability_weight': policy.durability_weight
        }

        selected_stores = self.cost_optimizer.select_optimal_stores(
            filtered_stores, data, metadata, optimization_criteria
        )

        return selected_stores

class CrossCloudConsistencyManager:
    """Manages consistency across cloud providers"""

    def __init__(self):
        self.vector_clocks = {}  # data_id -> VectorClock
        self.conflict_resolvers = {}  # data_type -> ConflictResolver
        self.sync_protocols = {}  # protocol_name -> SyncProtocol

    def ensure_consistency(self, data_id, updates, consistency_model='eventual'):
        """Ensure consistency for data updates across clouds"""
        if consistency_model == 'strong':
            return self._ensure_strong_consistency(data_id, updates)
        elif consistency_model == 'causal':
            return self._ensure_causal_consistency(data_id, updates)
        else:
            return self._ensure_eventual_consistency(data_id, updates)

    def _ensure_strong_consistency(self, data_id, updates):
        """Implement strong consistency using distributed locking"""
        # Acquire distributed lock across all replicas
        lock = self._acquire_cross_cloud_lock(data_id)

        try:
            # Apply updates atomically
            for update in updates:
                self._apply_update_atomically(data_id, update)

            # Ensure all replicas are synchronized
            self._synchronize_all_replicas(data_id)

        finally:
            lock.release()

    def _ensure_causal_consistency(self, data_id, updates):
        """Implement causal consistency using vector clocks"""
        current_clock = self.vector_clocks.get(data_id, VectorClock('system'))

        for update in updates:
            # Check causal dependencies
            if self._check_causal_dependencies(update, current_clock):
                # Apply update and increment clock
                self._apply_update_with_clock(data_id, update, current_clock)
                current_clock.tick()
            else:
                # Buffer update until dependencies are satisfied
                self._buffer_update(data_id, update)

    def resolve_conflicts(self, data_id, conflicting_versions):
        """Resolve conflicts between versions from different clouds"""
        data_type = self._get_data_type(data_id)
        resolver = self.conflict_resolvers.get(data_type)

        if not resolver:
            # Default: last-write-wins with timestamp
            return max(conflicting_versions, key=lambda v: v.timestamp)

        return resolver.resolve(conflicting_versions)
```

## Event-Driven Architecture at Scale

### Advanced Event Sourcing

```python
class EventSourcedAggregate:
    """Base class for event-sourced aggregates"""

    def __init__(self, aggregate_id):
        self.aggregate_id = aggregate_id
        self.version = 0
        self.uncommitted_events = []
        self.snapshot_frequency = 100  # Snapshot every 100 events

    def apply_event(self, event):
        """Apply event to aggregate state"""
        handler_name = f"_handle_{event.event_type.lower()}"
        handler = getattr(self, handler_name, None)

        if handler:
            handler(event)
        else:
            raise ValueError(f"No handler for event type: {event.event_type}")

        self.version += 1

    def raise_event(self, event_type, event_data):
        """Raise new event"""
        event = Event(
            aggregate_id=self.aggregate_id,
            event_type=event_type,
            event_data=event_data,
            version=self.version + 1,
            timestamp=datetime.utcnow()
        )

        # Apply event to current state
        self.apply_event(event)

        # Add to uncommitted events
        self.uncommitted_events.append(event)

    def mark_events_as_committed(self):
        """Mark events as committed after successful persistence"""
        self.uncommitted_events.clear()

    def create_snapshot(self):
        """Create snapshot of current state"""
        if self.version % self.snapshot_frequency == 0:
            return AggregateSnapshot(
                aggregate_id=self.aggregate_id,
                version=self.version,
                state=self._get_snapshot_state(),
                created_at=datetime.utcnow()
            )
        return None

class DistributedEventStore:
    """Distributed event store with partitioning and replication"""

    def __init__(self, partitions, replication_factor=3):
        self.partitions = partitions
        self.replication_factor = replication_factor
        self.partition_managers = {}
        self.consistency_manager = EventStoreConsistencyManager()

    def append_events(self, aggregate_id, events, expected_version):
        """Append events to event store with optimistic concurrency"""
        partition_id = self._get_partition(aggregate_id)
        partition_manager = self.partition_managers[partition_id]

        # Check optimistic concurrency
        current_version = partition_manager.get_current_version(aggregate_id)
        if current_version != expected_version:
            raise OptimisticConcurrencyException(
                f"Expected version {expected_version}, got {current_version}"
            )

        # Append events atomically
        return partition_manager.append_events_atomic(
            aggregate_id, events, expected_version
        )

    def get_events(self, aggregate_id, from_version=0):
        """Get events for aggregate starting from version"""
        partition_id = self._get_partition(aggregate_id)
        partition_manager = self.partition_managers[partition_id]

        # Check for snapshot first
        snapshot = partition_manager.get_latest_snapshot(aggregate_id)
        if snapshot and snapshot.version >= from_version:
            # Load from snapshot and get subsequent events
            events_after_snapshot = partition_manager.get_events(
                aggregate_id, snapshot.version + 1
            )
            return snapshot, events_after_snapshot
        else:
            # Load all events from specified version
            events = partition_manager.get_events(aggregate_id, from_version)
            return None, events

    def _get_partition(self, aggregate_id):
        """Determine partition for aggregate"""
        return hash(aggregate_id) % len(self.partitions)

class EventProjectionManager:
    """Manages read-side projections from event streams"""

    def __init__(self, event_store):
        self.event_store = event_store
        self.projections = {}  # projection_name -> Projection
        self.checkpoint_store = CheckpointStore()
        self.projection_workers = {}

    def register_projection(self, projection_name, projection_handler):
        """Register a new projection"""
        self.projections[projection_name] = projection_handler

        # Start projection worker
        worker = ProjectionWorker(
            projection_name, projection_handler,
            self.event_store, self.checkpoint_store
        )
        self.projection_workers[projection_name] = worker
        worker.start()

    def rebuild_projection(self, projection_name):
        """Rebuild projection from beginning"""
        if projection_name not in self.projections:
            raise ValueError(f"Unknown projection: {projection_name}")

        # Reset checkpoint
        self.checkpoint_store.reset_checkpoint(projection_name)

        # Restart worker
        self.projection_workers[projection_name].restart()

class ProjectionWorker:
    """Worker that processes events for a specific projection"""

    def __init__(self, projection_name, projection_handler, event_store, checkpoint_store):
        self.projection_name = projection_name
        self.projection_handler = projection_handler
        self.event_store = event_store
        self.checkpoint_store = checkpoint_store
        self.running = False
        self.batch_size = 1000

    def start(self):
        """Start processing events"""
        self.running = True

        # Get last checkpoint
        last_checkpoint = self.checkpoint_store.get_checkpoint(self.projection_name)

        while self.running:
            try:
                # Get batch of events
                events = self.event_store.get_events_from(
                    last_checkpoint, self.batch_size
                )

                if events:
                    # Process events in batch
                    self._process_event_batch(events)

                    # Update checkpoint
                    last_checkpoint = events[-1].global_position
                    self.checkpoint_store.update_checkpoint(
                        self.projection_name, last_checkpoint
                    )
                else:
                    # No new events, wait briefly
                    time.sleep(0.1)

            except Exception as e:
                self._handle_processing_error(e)

    def _process_event_batch(self, events):
        """Process a batch of events"""
        # Group events by aggregate for better performance
        events_by_aggregate = defaultdict(list)
        for event in events:
            events_by_aggregate[event.aggregate_id].append(event)

        # Process each aggregate's events
        for aggregate_id, aggregate_events in events_by_aggregate.items():
            try:
                self.projection_handler.handle_events(aggregate_events)
            except Exception as e:
                self._handle_event_processing_error(aggregate_id, aggregate_events, e)
```

### Saga Pattern Implementation

```python
class SagaOrchestrator:
    """Orchestrates long-running transactions using Saga pattern"""

    def __init__(self, event_bus, compensation_manager):
        self.event_bus = event_bus
        self.compensation_manager = compensation_manager
        self.active_sagas = {}  # saga_id -> SagaInstance
        self.saga_definitions = {}  # saga_type -> SagaDefinition

    def define_saga(self, saga_type, definition):
        """Define a saga workflow"""
        self.saga_definitions[saga_type] = definition

    def start_saga(self, saga_type, saga_data):
        """Start a new saga instance"""
        definition = self.saga_definitions.get(saga_type)
        if not definition:
            raise ValueError(f"Unknown saga type: {saga_type}")

        saga_id = self._generate_saga_id()
        saga_instance = SagaInstance(
            saga_id=saga_id,
            saga_type=saga_type,
            definition=definition,
            data=saga_data,
            orchestrator=self
        )

        self.active_sagas[saga_id] = saga_instance

        # Start executing the saga
        saga_instance.start()

        return saga_id

    def handle_step_completion(self, saga_id, step_name, result):
        """Handle completion of a saga step"""
        saga = self.active_sagas.get(saga_id)
        if not saga:
            return

        try:
            saga.handle_step_completion(step_name, result)
        except Exception as e:
            # Step failed, initiate compensation
            saga.initiate_compensation(step_name, e)

    def handle_step_failure(self, saga_id, step_name, error):
        """Handle failure of a saga step"""
        saga = self.active_sagas.get(saga_id)
        if saga:
            saga.initiate_compensation(step_name, error)

class SagaInstance:
    """Individual saga instance"""

    def __init__(self, saga_id, saga_type, definition, data, orchestrator):
        self.saga_id = saga_id
        self.saga_type = saga_type
        self.definition = definition
        self.data = data
        self.orchestrator = orchestrator
        self.current_step = 0
        self.completed_steps = []
        self.state = "RUNNING"
        self.compensation_stack = []

    def start(self):
        """Start executing the saga"""
        self._execute_next_step()

    def handle_step_completion(self, step_name, result):
        """Handle successful completion of a step"""
        step = self._get_current_step()
        if step.name != step_name:
            return  # Out of order completion

        # Store result and compensation info
        self.completed_steps.append({
            'step': step,
            'result': result,
            'compensation_data': step.get_compensation_data(result)
        })

        self.compensation_stack.append({
            'step': step,
            'compensation_data': step.get_compensation_data(result)
        })

        # Move to next step
        self.current_step += 1
        if self.current_step < len(self.definition.steps):
            self._execute_next_step()
        else:
            # Saga completed successfully
            self._complete_saga()

    def initiate_compensation(self, failed_step_name, error):
        """Initiate compensation for failed saga"""
        self.state = "COMPENSATING"

        # Execute compensation steps in reverse order
        while self.compensation_stack:
            compensation_info = self.compensation_stack.pop()
            try:
                self._execute_compensation_step(compensation_info)
            except Exception as comp_error:
                # Compensation failed - log and continue
                self._log_compensation_failure(compensation_info, comp_error)

        self.state = "COMPENSATED"
        self._cleanup_saga()

    def _execute_next_step(self):
        """Execute the next step in the saga"""
        step = self._get_current_step()

        try:
            # Execute step with timeout
            step_command = step.create_command(self.data)
            self.orchestrator.event_bus.publish_command(step_command)

        except Exception as e:
            self.initiate_compensation(step.name, e)

    def _execute_compensation_step(self, compensation_info):
        """Execute compensation for a completed step"""
        step = compensation_info['step']
        compensation_data = compensation_info['compensation_data']

        compensation_command = step.create_compensation_command(
            self.data, compensation_data
        )

        # Execute compensation synchronously (with retry)
        for attempt in range(3):
            try:
                self.orchestrator.event_bus.execute_command_sync(compensation_command)
                break
            except Exception as e:
                if attempt == 2:  # Last attempt
                    raise
                time.sleep(2 ** attempt)  # Exponential backoff

class SagaDefinition:
    """Definition of a saga workflow"""

    def __init__(self, saga_type):
        self.saga_type = saga_type
        self.steps = []

    def add_step(self, step):
        """Add a step to the saga"""
        self.steps.append(step)
        return self

class SagaStep:
    """Individual step in a saga"""

    def __init__(self, name, command_type, compensation_type):
        self.name = name
        self.command_type = command_type
        self.compensation_type = compensation_type

    def create_command(self, saga_data):
        """Create command for this step"""
        return self.command_type(saga_data)

    def create_compensation_command(self, saga_data, compensation_data):
        """Create compensation command for this step"""
        return self.compensation_type(saga_data, compensation_data)

    def get_compensation_data(self, step_result):
        """Extract compensation data from step result"""
        # Override in subclasses to extract relevant compensation data
        return step_result

# Example: E-commerce Order Saga
class OrderSagaDefinition(SagaDefinition):
    def __init__(self):
        super().__init__("order_processing")

        self.add_step(SagaStep(
            name="reserve_inventory",
            command_type=ReserveInventoryCommand,
            compensation_type=ReleaseInventoryCommand
        )).add_step(SagaStep(
            name="process_payment",
            command_type=ProcessPaymentCommand,
            compensation_type=RefundPaymentCommand
        )).add_step(SagaStep(
            name="ship_order",
            command_type=ShipOrderCommand,
            compensation_type=CancelShipmentCommand
        )).add_step(SagaStep(
            name="send_confirmation",
            command_type=SendConfirmationCommand,
            compensation_type=SendCancellationCommand
        ))
```

<function_calls>
<invoke name="TodoWrite">
<parameter name="todos">[{"content": "Create HLSD Advanced Concepts document", "status": "completed", "activeForm": "Creating HLSD Advanced Concepts document"}]