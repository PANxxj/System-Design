# High Level System Design (HLSD) - Intermediate Concepts

## Table of Contents
1. [Distributed System Fundamentals](#distributed-system-fundamentals)
2. [Data Partitioning and Sharding](#data-partitioning-and-sharding)
3. [Replication Strategies](#replication-strategies)
4. [Consistency Patterns](#consistency-patterns)
5. [Message Queues and Event Streaming](#message-queues-and-event-streaming)
6. [Service Discovery and Configuration](#service-discovery-and-configuration)
7. [API Gateway and Service Mesh](#api-gateway-and-service-mesh)
8. [Distributed Caching](#distributed-caching)
9. [Search and Indexing](#search-and-indexing)
10. [Real-world System Design Examples](#real-world-system-design-examples)

## Distributed System Fundamentals

### Distributed System Challenges

#### 1. Network Failures
- Partial network failures
- Network partitions
- Message loss and duplication

#### 2. Timing and Ordering
- No global clock
- Causal ordering of events
- Happens-before relationships

#### 3. Failure Detection
- Node failures vs network partitions
- Failure detection timeouts
- False positives/negatives

### Fallacies of Distributed Computing

1. **The network is reliable**
2. **Latency is zero**
3. **Bandwidth is infinite**
4. **The network is secure**
5. **Topology doesn't change**
6. **There is one administrator**
7. **Transport cost is zero**
8. **The network is homogeneous**

### Distributed System Models

#### 1. Client-Server Model
```
[Client 1] ↘
[Client 2] → [Server] → [Database]
[Client 3] ↗
```

#### 2. Peer-to-Peer Model
```
[Node A] ↔ [Node B]
   ↕         ↕
[Node D] ↔ [Node C]
```

#### 3. Microservices Model
```
[Client] → [API Gateway] → [Service A] → [DB A]
                         → [Service B] → [DB B]
                         → [Service C] → [DB C]
```

### Consensus Algorithms

#### Raft Consensus
```python
class RaftNode:
    def __init__(self, node_id, cluster_nodes):
        self.node_id = node_id
        self.cluster_nodes = cluster_nodes
        self.state = "FOLLOWER"  # FOLLOWER, CANDIDATE, LEADER
        self.current_term = 0
        self.voted_for = None
        self.log = []
        self.commit_index = 0

    def start_election(self):
        """Start leader election process"""
        self.state = "CANDIDATE"
        self.current_term += 1
        self.voted_for = self.node_id

        votes = 1  # Vote for self
        for node in self.cluster_nodes:
            if node != self.node_id:
                vote_response = self.request_vote(node)
                if vote_response:
                    votes += 1

        if votes > len(self.cluster_nodes) // 2:
            self.become_leader()

    def become_leader(self):
        """Become the leader"""
        self.state = "LEADER"
        # Send heartbeats to maintain leadership
        self.send_heartbeats()

    def request_vote(self, node):
        """Request vote from another node"""
        # In real implementation, this would be a network call
        return True  # Simplified

    def send_heartbeats(self):
        """Send heartbeat messages to followers"""
        for node in self.cluster_nodes:
            if node != self.node_id:
                # Send append_entries with empty log (heartbeat)
                pass
```

## Data Partitioning and Sharding

### Partitioning Strategies

#### 1. Horizontal Partitioning (Sharding)

**Range-based Partitioning**
```python
class RangePartitioner:
    def __init__(self, partitions):
        # partitions = [("A", "G"), ("H", "N"), ("O", "Z")]
        self.partitions = partitions

    def get_partition(self, key):
        first_char = key[0].upper()
        for i, (start, end) in enumerate(self.partitions):
            if start <= first_char <= end:
                return f"shard_{i}"
        return "shard_0"  # Default shard

# Usage
partitioner = RangePartitioner([("A", "G"), ("H", "N"), ("O", "Z")])
print(partitioner.get_partition("Alice"))  # shard_0
print(partitioner.get_partition("Mike"))   # shard_1
print(partitioner.get_partition("Zoe"))    # shard_2
```

**Hash-based Partitioning**
```python
import hashlib

class HashPartitioner:
    def __init__(self, num_partitions):
        self.num_partitions = num_partitions

    def get_partition(self, key):
        hash_value = int(hashlib.md5(key.encode()).hexdigest(), 16)
        return f"shard_{hash_value % self.num_partitions}"

# Usage
partitioner = HashPartitioner(4)
print(partitioner.get_partition("user_123"))  # shard_2
print(partitioner.get_partition("user_456"))  # shard_1
```

**Consistent Hashing**
```python
import hashlib
import bisect

class ConsistentHashRing:
    def __init__(self, nodes=None, replicas=3):
        self.replicas = replicas
        self.ring = {}
        self.sorted_keys = []

        if nodes:
            for node in nodes:
                self.add_node(node)

    def _hash(self, key):
        return int(hashlib.md5(key.encode()).hexdigest(), 16)

    def add_node(self, node):
        for i in range(self.replicas):
            key = self._hash(f"{node}:{i}")
            self.ring[key] = node
            self.sorted_keys.append(key)
        self.sorted_keys.sort()

    def remove_node(self, node):
        for i in range(self.replicas):
            key = self._hash(f"{node}:{i}")
            del self.ring[key]
            self.sorted_keys.remove(key)

    def get_node(self, key):
        if not self.ring:
            return None

        hash_key = self._hash(key)
        idx = bisect.bisect_right(self.sorted_keys, hash_key)

        if idx == len(self.sorted_keys):
            idx = 0

        return self.ring[self.sorted_keys[idx]]

# Usage
ring = ConsistentHashRing(['server1', 'server2', 'server3'])
print(ring.get_node('user_123'))  # server2
print(ring.get_node('user_456'))  # server1

# Adding a new server
ring.add_node('server4')
print(ring.get_node('user_123'))  # Might still be server2 (minimal reassignment)
```

#### 2. Vertical Partitioning
```sql
-- Split user table vertically
-- Basic user info
CREATE TABLE user_profile (
    user_id BIGINT PRIMARY KEY,
    username VARCHAR(50),
    email VARCHAR(100),
    created_at TIMESTAMP
);

-- Extended user info (less frequently accessed)
CREATE TABLE user_details (
    user_id BIGINT PRIMARY KEY,
    first_name VARCHAR(50),
    last_name VARCHAR(50),
    bio TEXT,
    profile_picture_url VARCHAR(255),
    FOREIGN KEY (user_id) REFERENCES user_profile(user_id)
);
```

### Sharding Challenges

#### 1. Rebalancing
```python
class ShardRebalancer:
    def __init__(self, hash_ring):
        self.hash_ring = hash_ring

    def add_shard(self, new_shard):
        """Add new shard and rebalance data"""
        # 1. Add new shard to hash ring
        self.hash_ring.add_node(new_shard)

        # 2. Identify data that needs to be moved
        affected_keys = self._find_affected_keys()

        # 3. Migrate data to new shard
        for key in affected_keys:
            old_shard = self._get_old_shard(key)
            new_shard_for_key = self.hash_ring.get_node(key)

            if old_shard != new_shard_for_key:
                self._migrate_data(key, old_shard, new_shard_for_key)

    def _migrate_data(self, key, from_shard, to_shard):
        """Migrate data from one shard to another"""
        # 1. Copy data to new shard
        data = self._get_data_from_shard(from_shard, key)
        self._write_data_to_shard(to_shard, key, data)

        # 2. Verify data integrity
        if self._verify_data_integrity(to_shard, key, data):
            # 3. Delete from old shard
            self._delete_data_from_shard(from_shard, key)
```

#### 2. Cross-Shard Queries
```python
class CrossShardQueryEngine:
    def __init__(self, shards):
        self.shards = shards

    def execute_cross_shard_query(self, query):
        """Execute query across multiple shards"""
        # 1. Determine which shards to query
        target_shards = self._determine_target_shards(query)

        # 2. Execute query on each shard in parallel
        results = []
        for shard in target_shards:
            shard_result = self._execute_on_shard(shard, query)
            results.append(shard_result)

        # 3. Merge and sort results
        return self._merge_results(results, query)

    def _merge_results(self, results, query):
        """Merge results from multiple shards"""
        merged = []
        for result in results:
            merged.extend(result)

        # Apply sorting if needed
        if query.has_order_by():
            merged.sort(key=lambda x: x[query.order_by_field])

        # Apply limit if needed
        if query.has_limit():
            merged = merged[:query.limit]

        return merged
```

## Replication Strategies

### Master-Slave Replication

```python
class MasterSlaveReplication:
    def __init__(self):
        self.master = None
        self.slaves = []
        self.replication_log = []

    def write(self, data):
        """Write to master and replicate to slaves"""
        # 1. Write to master
        log_entry = {
            'timestamp': time.time(),
            'operation': 'WRITE',
            'data': data,
            'log_sequence_number': len(self.replication_log)
        }

        self.master.write(data)
        self.replication_log.append(log_entry)

        # 2. Replicate to slaves asynchronously
        for slave in self.slaves:
            self._replicate_to_slave(slave, log_entry)

    def read(self, key, read_preference='slave'):
        """Read from master or slave based on preference"""
        if read_preference == 'master' or not self.slaves:
            return self.master.read(key)
        else:
            # Load balance across slaves
            slave = random.choice(self.slaves)
            return slave.read(key)

    def _replicate_to_slave(self, slave, log_entry):
        """Replicate log entry to slave"""
        try:
            slave.apply_log_entry(log_entry)
        except Exception as e:
            # Handle replication failure
            self._handle_replication_failure(slave, e)
```

### Master-Master Replication

```python
class MasterMasterReplication:
    def __init__(self, node_id):
        self.node_id = node_id
        self.peers = []
        self.vector_clock = {}

    def write(self, key, value):
        """Write with conflict detection"""
        # 1. Increment vector clock
        self.vector_clock[self.node_id] = self.vector_clock.get(self.node_id, 0) + 1

        # 2. Create versioned entry
        entry = {
            'key': key,
            'value': value,
            'timestamp': time.time(),
            'node_id': self.node_id,
            'vector_clock': self.vector_clock.copy()
        }

        # 3. Write locally
        self._write_local(entry)

        # 4. Propagate to peers
        for peer in self.peers:
            self._propagate_to_peer(peer, entry)

    def _resolve_conflict(self, local_entry, remote_entry):
        """Resolve write conflicts"""
        # Last-writer-wins based on timestamp
        if remote_entry['timestamp'] > local_entry['timestamp']:
            return remote_entry
        elif remote_entry['timestamp'] < local_entry['timestamp']:
            return local_entry
        else:
            # Tie-breaker: node ID
            if remote_entry['node_id'] > local_entry['node_id']:
                return remote_entry
            return local_entry
```

## Consistency Patterns

### Consistency Models

#### 1. Strong Consistency
- All nodes see the same data at the same time
- Achieved through synchronous replication
- Higher latency, lower availability

```python
class StrongConsistencyStore:
    def __init__(self, nodes):
        self.nodes = nodes
        self.master = nodes[0]

    def write(self, key, value):
        """Synchronous write to all nodes"""
        # 1. Write to master
        self.master.write(key, value)

        # 2. Synchronously replicate to all replicas
        for node in self.nodes[1:]:
            try:
                node.write(key, value)
            except Exception:
                # If any replica fails, abort the write
                self._abort_write(key)
                raise Exception("Write failed - consistency violated")

        return True

    def read(self, key):
        """Read from master for consistency"""
        return self.master.read(key)
```

#### 2. Eventual Consistency
- Nodes will eventually converge to the same state
- Temporary inconsistencies allowed
- Higher availability, lower latency

```python
class EventualConsistencyStore:
    def __init__(self, nodes):
        self.nodes = nodes
        self.replication_queue = queue.Queue()
        self._start_replication_worker()

    def write(self, key, value):
        """Asynchronous write with eventual consistency"""
        # 1. Write to local node immediately
        local_node = random.choice(self.nodes)
        local_node.write(key, value)

        # 2. Queue replication to other nodes
        for node in self.nodes:
            if node != local_node:
                self.replication_queue.put((node, key, value))

        return True

    def _replication_worker(self):
        """Background worker for replication"""
        while True:
            try:
                node, key, value = self.replication_queue.get(timeout=1)
                node.write(key, value)
                self.replication_queue.task_done()
            except queue.Empty:
                continue
```

#### 3. Causal Consistency
- Preserves causal relationships between operations
- If A causally precedes B, all nodes see A before B

```python
class CausalConsistencyStore:
    def __init__(self, node_id):
        self.node_id = node_id
        self.vector_clock = {}
        self.data = {}

    def write(self, key, value, causal_context=None):
        """Write with causal ordering"""
        # Update vector clock
        self.vector_clock[self.node_id] = self.vector_clock.get(self.node_id, 0) + 1

        # Merge causal context if provided
        if causal_context:
            for node, clock in causal_context.items():
                self.vector_clock[node] = max(
                    self.vector_clock.get(node, 0), clock
                )

        # Store with vector clock
        self.data[key] = {
            'value': value,
            'vector_clock': self.vector_clock.copy()
        }

    def read(self, key):
        """Read with causal context"""
        if key in self.data:
            return {
                'value': self.data[key]['value'],
                'causal_context': self.data[key]['vector_clock']
            }
        return None
```

### BASE Properties (Eventual Consistency)

- **Basically Available**: System remains available
- **Soft State**: State may change over time
- **Eventual Consistency**: System will become consistent over time

## Message Queues and Event Streaming

### Message Queue Patterns

#### 1. Point-to-Point Queue
```python
import queue
import threading

class MessageQueue:
    def __init__(self):
        self.queue = queue.Queue()
        self.consumers = []

    def publish(self, message):
        """Send message to queue"""
        self.queue.put(message)

    def subscribe(self, consumer):
        """Register consumer"""
        self.consumers.append(consumer)
        # Start consumer thread
        thread = threading.Thread(target=self._consume, args=(consumer,))
        thread.daemon = True
        thread.start()

    def _consume(self, consumer):
        """Consume messages for a specific consumer"""
        while True:
            try:
                message = self.queue.get(timeout=1)
                consumer.process(message)
                self.queue.task_done()
            except queue.Empty:
                continue

class Consumer:
    def __init__(self, name):
        self.name = name

    def process(self, message):
        print(f"{self.name} processing: {message}")
```

#### 2. Publish-Subscribe
```python
class PubSubSystem:
    def __init__(self):
        self.topics = {}

    def create_topic(self, topic_name):
        """Create a new topic"""
        if topic_name not in self.topics:
            self.topics[topic_name] = {
                'subscribers': [],
                'messages': []
            }

    def subscribe(self, topic_name, subscriber):
        """Subscribe to a topic"""
        if topic_name in self.topics:
            self.topics[topic_name]['subscribers'].append(subscriber)

    def publish(self, topic_name, message):
        """Publish message to topic"""
        if topic_name in self.topics:
            # Store message
            self.topics[topic_name]['messages'].append(message)

            # Notify all subscribers
            for subscriber in self.topics[topic_name]['subscribers']:
                try:
                    subscriber.on_message(message)
                except Exception as e:
                    print(f"Error notifying subscriber: {e}")

class Subscriber:
    def __init__(self, name):
        self.name = name

    def on_message(self, message):
        print(f"{self.name} received: {message}")

# Usage
pubsub = PubSubSystem()
pubsub.create_topic('user_events')

subscriber1 = Subscriber('Email Service')
subscriber2 = Subscriber('Analytics Service')

pubsub.subscribe('user_events', subscriber1)
pubsub.subscribe('user_events', subscriber2)

pubsub.publish('user_events', {'event': 'user_signup', 'user_id': '123'})
```

### Event Streaming

#### Kafka-like Event Streaming
```python
import time
from collections import defaultdict

class EventStream:
    def __init__(self):
        self.partitions = defaultdict(list)  # partition_id -> list of events
        self.consumer_offsets = defaultdict(int)  # consumer_id -> offset

    def produce(self, topic, partition, event):
        """Produce event to partition"""
        event_with_metadata = {
            'event': event,
            'timestamp': time.time(),
            'offset': len(self.partitions[f"{topic}:{partition}"])
        }
        self.partitions[f"{topic}:{partition}"].append(event_with_metadata)

    def consume(self, consumer_id, topic, partition, batch_size=10):
        """Consume events from partition"""
        partition_key = f"{topic}:{partition}"
        consumer_key = f"{consumer_id}:{partition_key}"

        current_offset = self.consumer_offsets[consumer_key]
        partition_events = self.partitions[partition_key]

        # Get batch of events
        batch = partition_events[current_offset:current_offset + batch_size]

        # Update consumer offset
        self.consumer_offsets[consumer_key] = current_offset + len(batch)

        return batch

    def get_consumer_lag(self, consumer_id, topic, partition):
        """Get consumer lag (how far behind the consumer is)"""
        partition_key = f"{topic}:{partition}"
        consumer_key = f"{consumer_id}:{partition_key}"

        latest_offset = len(self.partitions[partition_key])
        consumer_offset = self.consumer_offsets[consumer_key]

        return latest_offset - consumer_offset

# Usage
stream = EventStream()

# Produce events
for i in range(100):
    stream.produce('user_events', 0, {'user_id': i, 'action': 'login'})

# Consume events
consumer_id = 'analytics_service'
batch = stream.consume(consumer_id, 'user_events', 0, batch_size=10)
print(f"Consumed {len(batch)} events")

# Check consumer lag
lag = stream.get_consumer_lag(consumer_id, 'user_events', 0)
print(f"Consumer lag: {lag} events")
```

### Message Delivery Guarantees

#### 1. At-Most-Once
```python
class AtMostOnceQueue:
    def __init__(self):
        self.queue = queue.Queue()

    def send(self, message):
        """Send message (may be lost)"""
        try:
            self.queue.put_nowait(message)
            return True
        except queue.Full:
            return False  # Message lost

    def receive(self):
        """Receive message (acknowledge immediately)"""
        try:
            message = self.queue.get_nowait()
            # Immediate acknowledgment - message could be lost if processing fails
            return message
        except queue.Empty:
            return None
```

#### 2. At-Least-Once
```python
class AtLeastOnceQueue:
    def __init__(self):
        self.queue = queue.Queue()
        self.pending_acks = {}
        self.message_id = 0

    def send(self, message):
        """Send message with retry capability"""
        msg_id = self.message_id
        self.message_id += 1

        message_with_id = {
            'id': msg_id,
            'content': message,
            'retry_count': 0
        }

        self.queue.put(message_with_id)
        return msg_id

    def receive(self):
        """Receive message (must be acknowledged)"""
        message = self.queue.get()
        self.pending_acks[message['id']] = message
        return message

    def acknowledge(self, message_id):
        """Acknowledge message processing"""
        if message_id in self.pending_acks:
            del self.pending_acks[message_id]

    def retry_unacknowledged(self):
        """Retry unacknowledged messages"""
        for msg_id, message in self.pending_acks.items():
            message['retry_count'] += 1
            if message['retry_count'] < 3:  # Max retries
                self.queue.put(message)
```

#### 3. Exactly-Once
```python
class ExactlyOnceQueue:
    def __init__(self):
        self.queue = queue.Queue()
        self.processed_messages = set()
        self.pending_acks = {}

    def send(self, message, idempotency_key):
        """Send message with idempotency key"""
        if idempotency_key not in self.processed_messages:
            self.queue.put({
                'content': message,
                'idempotency_key': idempotency_key
            })

    def receive(self):
        """Receive message (idempotent processing)"""
        message = self.queue.get()
        idempotency_key = message['idempotency_key']

        if idempotency_key in self.processed_messages:
            # Already processed, skip
            return None

        self.pending_acks[idempotency_key] = message
        return message

    def acknowledge(self, idempotency_key):
        """Acknowledge and mark as processed"""
        if idempotency_key in self.pending_acks:
            self.processed_messages.add(idempotency_key)
            del self.pending_acks[idempotency_key]
```

## Service Discovery and Configuration

### Service Discovery Patterns

#### 1. Client-Side Discovery
```python
class ServiceRegistry:
    def __init__(self):
        self.services = {}

    def register(self, service_name, instance):
        """Register service instance"""
        if service_name not in self.services:
            self.services[service_name] = []
        self.services[service_name].append(instance)

    def discover(self, service_name):
        """Discover service instances"""
        return self.services.get(service_name, [])

    def deregister(self, service_name, instance):
        """Deregister service instance"""
        if service_name in self.services:
            self.services[service_name].remove(instance)

class ServiceClient:
    def __init__(self, service_registry):
        self.registry = service_registry
        self.load_balancer = RoundRobinLoadBalancer()

    def call_service(self, service_name, method, data):
        """Call service with client-side discovery"""
        # 1. Discover service instances
        instances = self.registry.discover(service_name)
        if not instances:
            raise Exception(f"No instances found for {service_name}")

        # 2. Load balance
        instance = self.load_balancer.select(instances)

        # 3. Make service call
        return self._make_http_call(instance, method, data)

class RoundRobinLoadBalancer:
    def __init__(self):
        self.counters = {}

    def select(self, instances):
        service_key = str(sorted(instances))
        if service_key not in self.counters:
            self.counters[service_key] = 0

        instance = instances[self.counters[service_key] % len(instances)]
        self.counters[service_key] += 1
        return instance
```

#### 2. Server-Side Discovery
```python
class LoadBalancer:
    def __init__(self, service_registry):
        self.registry = service_registry
        self.health_checker = HealthChecker()

    def route_request(self, service_name, request):
        """Route request to healthy service instance"""
        # 1. Get healthy instances
        instances = self.registry.discover(service_name)
        healthy_instances = [
            instance for instance in instances
            if self.health_checker.is_healthy(instance)
        ]

        if not healthy_instances:
            raise Exception(f"No healthy instances for {service_name}")

        # 2. Select instance
        instance = self._select_instance(healthy_instances)

        # 3. Forward request
        return self._forward_request(instance, request)

class HealthChecker:
    def __init__(self):
        self.health_cache = {}
        self.cache_ttl = 30  # seconds

    def is_healthy(self, instance):
        """Check if service instance is healthy"""
        cache_key = f"{instance['host']}:{instance['port']}"

        # Check cache first
        if cache_key in self.health_cache:
            cached_time, is_healthy = self.health_cache[cache_key]
            if time.time() - cached_time < self.cache_ttl:
                return is_healthy

        # Perform health check
        try:
            response = requests.get(
                f"http://{instance['host']}:{instance['port']}/health",
                timeout=5
            )
            is_healthy = response.status_code == 200
        except:
            is_healthy = False

        # Cache result
        self.health_cache[cache_key] = (time.time(), is_healthy)
        return is_healthy
```

### Configuration Management

#### 1. Centralized Configuration
```python
class ConfigurationService:
    def __init__(self):
        self.configs = {}
        self.watchers = defaultdict(list)

    def set_config(self, key, value):
        """Set configuration value"""
        old_value = self.configs.get(key)
        self.configs[key] = value

        # Notify watchers if value changed
        if old_value != value:
            self._notify_watchers(key, value)

    def get_config(self, key, default=None):
        """Get configuration value"""
        return self.configs.get(key, default)

    def watch(self, key, callback):
        """Watch for configuration changes"""
        self.watchers[key].append(callback)

    def _notify_watchers(self, key, value):
        """Notify watchers of configuration change"""
        for callback in self.watchers[key]:
            try:
                callback(key, value)
            except Exception as e:
                print(f"Error notifying watcher: {e}")

class ServiceWithDynamicConfig:
    def __init__(self, config_service):
        self.config_service = config_service
        self.rate_limit = config_service.get_config('rate_limit', 100)

        # Watch for configuration changes
        config_service.watch('rate_limit', self._on_rate_limit_change)

    def _on_rate_limit_change(self, key, value):
        """Handle rate limit configuration change"""
        print(f"Rate limit changed from {self.rate_limit} to {value}")
        self.rate_limit = value

    def process_request(self, request):
        """Process request with dynamic rate limiting"""
        if self._is_rate_limited():
            raise Exception("Rate limit exceeded")

        # Process request
        return "Request processed"

    def _is_rate_limited(self):
        # Implement rate limiting logic using self.rate_limit
        return False
```

## API Gateway and Service Mesh

### API Gateway Implementation

```python
class APIGateway:
    def __init__(self):
        self.routes = {}
        self.middleware_stack = []
        self.service_registry = ServiceRegistry()

    def add_route(self, path_pattern, service_name, methods=None):
        """Add route mapping"""
        self.routes[path_pattern] = {
            'service_name': service_name,
            'methods': methods or ['GET', 'POST', 'PUT', 'DELETE']
        }

    def add_middleware(self, middleware):
        """Add middleware to the stack"""
        self.middleware_stack.append(middleware)

    def handle_request(self, request):
        """Handle incoming request"""
        # 1. Apply middleware
        for middleware in self.middleware_stack:
            response = middleware.process_request(request)
            if response:  # Middleware short-circuited
                return response

        # 2. Route to service
        route = self._find_route(request.path)
        if not route:
            return Response(404, {'error': 'Route not found'})

        # 3. Discover service instance
        instances = self.service_registry.discover(route['service_name'])
        if not instances:
            return Response(503, {'error': 'Service unavailable'})

        # 4. Load balance and forward
        instance = self._select_instance(instances)
        response = self._forward_request(instance, request)

        # 5. Apply response middleware
        for middleware in reversed(self.middleware_stack):
            response = middleware.process_response(request, response)

        return response

class RateLimitMiddleware:
    def __init__(self, requests_per_minute=60):
        self.requests_per_minute = requests_per_minute
        self.client_requests = defaultdict(list)

    def process_request(self, request):
        """Apply rate limiting"""
        client_ip = request.get_client_ip()
        current_time = time.time()

        # Clean old requests
        self.client_requests[client_ip] = [
            req_time for req_time in self.client_requests[client_ip]
            if current_time - req_time < 60
        ]

        # Check rate limit
        if len(self.client_requests[client_ip]) >= self.requests_per_minute:
            return Response(429, {'error': 'Rate limit exceeded'})

        # Add current request
        self.client_requests[client_ip].append(current_time)
        return None  # Continue processing

    def process_response(self, request, response):
        """Process response (no-op for rate limiting)"""
        return response

class AuthenticationMiddleware:
    def __init__(self, auth_service):
        self.auth_service = auth_service

    def process_request(self, request):
        """Authenticate request"""
        auth_header = request.headers.get('Authorization')
        if not auth_header:
            return Response(401, {'error': 'Authentication required'})

        token = auth_header.replace('Bearer ', '')
        user = self.auth_service.validate_token(token)

        if not user:
            return Response(401, {'error': 'Invalid token'})

        # Add user to request context
        request.user = user
        return None

    def process_response(self, request, response):
        return response
```

### Service Mesh (Sidecar Pattern)

```python
class ServiceMeshSidecar:
    def __init__(self, service_name, service_registry):
        self.service_name = service_name
        self.service_registry = service_registry
        self.circuit_breakers = {}
        self.metrics_collector = MetricsCollector()

    def make_service_call(self, target_service, endpoint, data=None):
        """Make service call through service mesh"""
        # 1. Service discovery
        instances = self.service_registry.discover(target_service)
        if not instances:
            raise ServiceUnavailableError(f"No instances for {target_service}")

        # 2. Load balancing
        instance = self._select_instance(instances)

        # 3. Circuit breaker
        circuit_breaker = self._get_circuit_breaker(target_service)
        if circuit_breaker.is_open():
            raise CircuitBreakerOpenError(f"Circuit breaker open for {target_service}")

        # 4. Make call with retries and timeout
        try:
            response = self._make_http_call_with_retry(instance, endpoint, data)
            circuit_breaker.record_success()
            self.metrics_collector.record_success(target_service)
            return response
        except Exception as e:
            circuit_breaker.record_failure()
            self.metrics_collector.record_failure(target_service)
            raise

    def _get_circuit_breaker(self, service_name):
        """Get or create circuit breaker for service"""
        if service_name not in self.circuit_breakers:
            self.circuit_breakers[service_name] = CircuitBreaker(
                failure_threshold=5,
                recovery_timeout=30
            )
        return self.circuit_breakers[service_name]

class CircuitBreaker:
    def __init__(self, failure_threshold=5, recovery_timeout=60):
        self.failure_threshold = failure_threshold
        self.recovery_timeout = recovery_timeout
        self.failure_count = 0
        self.last_failure_time = None
        self.state = 'CLOSED'  # CLOSED, OPEN, HALF_OPEN

    def is_open(self):
        """Check if circuit breaker is open"""
        if self.state == 'OPEN':
            if self._should_attempt_reset():
                self.state = 'HALF_OPEN'
                return False
            return True
        return False

    def record_success(self):
        """Record successful call"""
        self.failure_count = 0
        self.state = 'CLOSED'

    def record_failure(self):
        """Record failed call"""
        self.failure_count += 1
        self.last_failure_time = time.time()

        if self.failure_count >= self.failure_threshold:
            self.state = 'OPEN'

    def _should_attempt_reset(self):
        """Check if we should attempt to reset circuit breaker"""
        return (time.time() - self.last_failure_time) >= self.recovery_timeout

class MetricsCollector:
    def __init__(self):
        self.metrics = defaultdict(lambda: {'success': 0, 'failure': 0, 'latency': []})

    def record_success(self, service_name, latency=None):
        """Record successful service call"""
        self.metrics[service_name]['success'] += 1
        if latency:
            self.metrics[service_name]['latency'].append(latency)

    def record_failure(self, service_name):
        """Record failed service call"""
        self.metrics[service_name]['failure'] += 1

    def get_success_rate(self, service_name):
        """Calculate success rate for service"""
        metrics = self.metrics[service_name]
        total = metrics['success'] + metrics['failure']
        if total == 0:
            return 1.0
        return metrics['success'] / total

    def get_average_latency(self, service_name):
        """Calculate average latency for service"""
        latencies = self.metrics[service_name]['latency']
        if not latencies:
            return 0
        return sum(latencies) / len(latencies)
```

## Distributed Caching

### Cache Invalidation Strategies

#### 1. Time-Based Expiration (TTL)
```python
import time

class TTLCache:
    def __init__(self):
        self.cache = {}
        self.expiry_times = {}

    def set(self, key, value, ttl_seconds):
        """Set value with TTL"""
        self.cache[key] = value
        self.expiry_times[key] = time.time() + ttl_seconds

    def get(self, key):
        """Get value if not expired"""
        if key not in self.cache:
            return None

        if time.time() > self.expiry_times[key]:
            # Expired
            del self.cache[key]
            del self.expiry_times[key]
            return None

        return self.cache[key]

    def cleanup_expired(self):
        """Remove expired entries"""
        current_time = time.time()
        expired_keys = [
            key for key, expiry_time in self.expiry_times.items()
            if current_time > expiry_time
        ]

        for key in expired_keys:
            del self.cache[key]
            del self.expiry_times[key]
```

#### 2. Event-Based Invalidation
```python
class EventBasedCache:
    def __init__(self, event_bus):
        self.cache = {}
        self.key_dependencies = defaultdict(set)  # key -> set of entities
        self.entity_keys = defaultdict(set)       # entity -> set of keys

        # Subscribe to invalidation events
        event_bus.subscribe('entity_updated', self._handle_entity_update)
        event_bus.subscribe('entity_deleted', self._handle_entity_delete)

    def set(self, key, value, dependencies=None):
        """Set value with optional dependencies"""
        self.cache[key] = value

        if dependencies:
            self.key_dependencies[key] = set(dependencies)
            for entity in dependencies:
                self.entity_keys[entity].add(key)

    def get(self, key):
        """Get cached value"""
        return self.cache.get(key)

    def _handle_entity_update(self, entity_id):
        """Handle entity update event"""
        keys_to_invalidate = self.entity_keys.get(entity_id, set())
        for key in keys_to_invalidate:
            self._invalidate_key(key)

    def _handle_entity_delete(self, entity_id):
        """Handle entity deletion event"""
        self._handle_entity_update(entity_id)  # Same logic

    def _invalidate_key(self, key):
        """Invalidate specific cache key"""
        if key in self.cache:
            del self.cache[key]

            # Clean up dependency tracking
            dependencies = self.key_dependencies.get(key, set())
            for entity in dependencies:
                self.entity_keys[entity].discard(key)

            if key in self.key_dependencies:
                del self.key_dependencies[key]

# Usage
event_bus = EventBus()
cache = EventBasedCache(event_bus)

# Cache user profile with dependencies
cache.set('user_profile:123', user_data, dependencies=['user:123'])

# When user is updated, cache is automatically invalidated
event_bus.publish('entity_updated', 'user:123')
```

### Cache Warming Strategies

```python
class CacheWarmer:
    def __init__(self, cache, data_source):
        self.cache = cache
        self.data_source = data_source

    def warm_cache_on_startup(self):
        """Warm cache during application startup"""
        print("Warming cache on startup...")

        # Warm popular data
        popular_items = self.data_source.get_popular_items(limit=1000)
        for item in popular_items:
            self.cache.set(f"item:{item.id}", item, ttl_seconds=3600)

    def warm_cache_predictively(self):
        """Warm cache based on predictions"""
        # Analyze access patterns
        trending_items = self.data_source.get_trending_items()
        for item in trending_items:
            if not self.cache.get(f"item:{item.id}"):
                # Pre-load trending items
                self.cache.set(f"item:{item.id}", item, ttl_seconds=3600)

    def warm_cache_on_miss(self, key, loader_func):
        """Warm cache on cache miss"""
        value = self.cache.get(key)
        if value is None:
            # Cache miss - load and cache
            value = loader_func()
            if value:
                self.cache.set(key, value, ttl_seconds=3600)
        return value
```

## Search and Indexing

### Full-Text Search Implementation

```python
import re
from collections import defaultdict
import math

class InvertedIndex:
    def __init__(self):
        self.index = defaultdict(set)  # term -> set of document IDs
        self.documents = {}            # doc_id -> document content
        self.term_frequencies = defaultdict(lambda: defaultdict(int))  # doc_id -> term -> frequency
        self.document_lengths = {}     # doc_id -> document length

    def add_document(self, doc_id, content):
        """Add document to index"""
        self.documents[doc_id] = content
        terms = self._tokenize(content)
        self.document_lengths[doc_id] = len(terms)

        # Build inverted index
        for term in terms:
            self.index[term].add(doc_id)
            self.term_frequencies[doc_id][term] += 1

    def search(self, query, limit=10):
        """Search documents using TF-IDF scoring"""
        query_terms = self._tokenize(query)
        if not query_terms:
            return []

        # Find candidate documents
        candidate_docs = set()
        for term in query_terms:
            candidate_docs.update(self.index.get(term, set()))

        # Score documents
        scored_docs = []
        for doc_id in candidate_docs:
            score = self._calculate_tfidf_score(doc_id, query_terms)
            scored_docs.append((doc_id, score))

        # Sort by score and return top results
        scored_docs.sort(key=lambda x: x[1], reverse=True)
        return scored_docs[:limit]

    def _tokenize(self, text):
        """Tokenize text into terms"""
        # Simple tokenization - split on whitespace and punctuation
        terms = re.findall(r'\b\w+\b', text.lower())
        return terms

    def _calculate_tfidf_score(self, doc_id, query_terms):
        """Calculate TF-IDF score for document"""
        score = 0.0
        total_docs = len(self.documents)

        for term in query_terms:
            if term in self.term_frequencies[doc_id]:
                # Term Frequency
                tf = self.term_frequencies[doc_id][term] / self.document_lengths[doc_id]

                # Inverse Document Frequency
                docs_with_term = len(self.index[term])
                idf = math.log(total_docs / docs_with_term) if docs_with_term > 0 else 0

                # TF-IDF
                score += tf * idf

        return score

# Usage
search_index = InvertedIndex()

# Add documents
search_index.add_document(1, "Python is a great programming language")
search_index.add_document(2, "Java programming language is also popular")
search_index.add_document(3, "Machine learning with Python is powerful")

# Search
results = search_index.search("Python programming")
for doc_id, score in results:
    print(f"Document {doc_id}: {score:.4f}")
```

### Faceted Search

```python
class FacetedSearchIndex:
    def __init__(self):
        self.documents = {}
        self.facet_index = defaultdict(lambda: defaultdict(set))  # facet -> value -> doc_ids

    def add_document(self, doc_id, content, facets):
        """Add document with facets"""
        self.documents[doc_id] = {
            'content': content,
            'facets': facets
        }

        # Index facets
        for facet_name, facet_value in facets.items():
            if isinstance(facet_value, list):
                for value in facet_value:
                    self.facet_index[facet_name][value].add(doc_id)
            else:
                self.facet_index[facet_name][facet_value].add(doc_id)

    def search_with_facets(self, query=None, facet_filters=None):
        """Search with facet filtering"""
        # Start with all documents
        candidate_docs = set(self.documents.keys())

        # Apply facet filters
        if facet_filters:
            for facet_name, facet_values in facet_filters.items():
                if not isinstance(facet_values, list):
                    facet_values = [facet_values]

                facet_docs = set()
                for value in facet_values:
                    facet_docs.update(self.facet_index[facet_name].get(value, set()))

                candidate_docs.intersection_update(facet_docs)

        # Apply text search if query provided
        if query:
            # Simplified text matching
            query_lower = query.lower()
            text_matching_docs = {
                doc_id for doc_id, doc in self.documents.items()
                if query_lower in doc['content'].lower()
            }
            candidate_docs.intersection_update(text_matching_docs)

        # Calculate facet counts for remaining documents
        facet_counts = self._calculate_facet_counts(candidate_docs)

        return {
            'documents': list(candidate_docs),
            'facet_counts': facet_counts,
            'total_count': len(candidate_docs)
        }

    def _calculate_facet_counts(self, doc_ids):
        """Calculate facet counts for given documents"""
        facet_counts = defaultdict(lambda: defaultdict(int))

        for doc_id in doc_ids:
            doc_facets = self.documents[doc_id]['facets']
            for facet_name, facet_value in doc_facets.items():
                if isinstance(facet_value, list):
                    for value in facet_value:
                        facet_counts[facet_name][value] += 1
                else:
                    facet_counts[facet_name][facet_value] += 1

        return dict(facet_counts)

# Usage
faceted_index = FacetedSearchIndex()

# Add products with facets
faceted_index.add_document(1, "iPhone 13 Pro", {
    'brand': 'Apple',
    'category': 'Electronics',
    'price_range': '800-1000',
    'features': ['5G', 'Face ID', 'Wireless Charging']
})

faceted_index.add_document(2, "Samsung Galaxy S21", {
    'brand': 'Samsung',
    'category': 'Electronics',
    'price_range': '600-800',
    'features': ['5G', 'Fingerprint', 'Wireless Charging']
})

# Search with facets
results = faceted_index.search_with_facets(
    query="phone",
    facet_filters={
        'category': 'Electronics',
        'features': '5G'
    }
)

print(f"Found {results['total_count']} products")
print("Facet counts:", results['facet_counts'])
```

## Real-world System Design Examples

### Example 1: Chat System Design

```python
# High-level architecture for a chat system

class ChatSystemArchitecture:
    """
    Components:
    1. API Gateway - Entry point for all requests
    2. User Service - User management and authentication
    3. Chat Service - Message handling and chat rooms
    4. Message Queue - Async message processing
    5. WebSocket Service - Real-time communication
    6. Notification Service - Push notifications
    7. Media Service - File/image sharing
    8. Database - Message storage and user data
    9. Cache - Online users and recent messages
    """

    def __init__(self):
        self.components = {
            'api_gateway': 'Routes requests, rate limiting, auth',
            'user_service': 'User CRUD, authentication, profiles',
            'chat_service': 'Chat rooms, message routing',
            'websocket_service': 'Real-time connections',
            'message_queue': 'Async processing (Kafka/RabbitMQ)',
            'notification_service': 'Push notifications',
            'media_service': 'File uploads and CDN',
            'database': 'PostgreSQL for users, Cassandra for messages',
            'cache': 'Redis for online users and recent messages'
        }

    def get_capacity_estimates(self):
        """
        Capacity Estimation:
        - 500M daily active users
        - 40 messages per user per day
        - 20B messages per day
        - 200:1 read/write ratio (viewing vs sending)

        Write QPS: 20B / (24 * 3600) = ~230K messages/second
        Read QPS: 230K * 200 = 46M reads/second

        Storage:
        - Average message size: 100 bytes
        - Daily: 20B * 100 bytes = 2TB/day
        - 5 years: 2TB * 365 * 5 = ~3.6PB
        """
        return {
            'daily_active_users': '500M',
            'daily_messages': '20B',
            'write_qps': '230K/sec',
            'read_qps': '46M/sec',
            'daily_storage': '2TB',
            'five_year_storage': '3.6PB'
        }

    def get_database_design(self):
        """
        Database Schema:

        Users (PostgreSQL):
        - user_id, username, email, password_hash, created_at

        Chat Rooms (PostgreSQL):
        - room_id, room_name, room_type (direct/group), created_by, created_at

        Room Members (PostgreSQL):
        - room_id, user_id, joined_at, role (admin/member)

        Messages (Cassandra - partitioned by room_id):
        - message_id, room_id, user_id, content, message_type, created_at
        - Partition key: room_id
        - Clustering key: created_at (for chronological ordering)
        """
        return {
            'users_db': 'PostgreSQL - ACID compliance for user data',
            'messages_db': 'Cassandra - Scalable, time-series optimized',
            'cache': 'Redis - Online users, recent messages'
        }

class WebSocketConnectionManager:
    """Manages WebSocket connections for real-time chat"""

    def __init__(self):
        self.connections = {}  # user_id -> websocket connection
        self.user_rooms = defaultdict(set)  # user_id -> set of room_ids

    def connect_user(self, user_id, websocket):
        """Connect user and load their rooms"""
        self.connections[user_id] = websocket
        # Load user's rooms from database
        rooms = self._get_user_rooms(user_id)
        self.user_rooms[user_id] = rooms

    def send_message_to_room(self, room_id, message, sender_id):
        """Send message to all users in a room"""
        room_members = self._get_room_members(room_id)
        for member_id in room_members:
            if member_id != sender_id and member_id in self.connections:
                try:
                    self.connections[member_id].send(message)
                except:
                    # Connection broken, remove it
                    self._remove_connection(member_id)

    def _get_user_rooms(self, user_id):
        # Query database for user's rooms
        pass

    def _get_room_members(self, room_id):
        # Query database for room members
        pass
```

### Example 2: Ride-Sharing System Design

```python
class RideSharingSystemArchitecture:
    """
    Uber/Lyft-like ride-sharing system architecture

    Components:
    1. User Service - Riders and drivers
    2. Location Service - Real-time location tracking
    3. Matching Service - Match riders with drivers
    4. Trip Service - Trip management
    5. Payment Service - Payment processing
    6. Notification Service - Real-time updates
    7. Maps Service - Route calculation and ETA
    """

    def get_capacity_estimates(self):
        """
        Capacity Estimation:
        - 300M users (10M drivers, 290M riders)
        - 1M active drivers at peak
        - 10M rides per day
        - 3 location updates per minute per active driver

        Location updates: 1M * 3 * 60 * 24 = 4.3B updates/day
        Location update QPS: 4.3B / 86400 = ~50K/sec

        Trip requests: 10M / 86400 = ~115/sec
        """
        return {
            'total_users': '300M',
            'daily_rides': '10M',
            'location_updates': '4.3B/day',
            'location_qps': '50K/sec',
            'trip_requests_qps': '115/sec'
        }

class LocationService:
    """Handles real-time location tracking"""

    def __init__(self):
        self.driver_locations = {}  # driver_id -> (lat, lon, timestamp)
        self.location_index = QuadTree()  # Spatial index for fast proximity search

    def update_driver_location(self, driver_id, latitude, longitude):
        """Update driver location"""
        timestamp = time.time()
        self.driver_locations[driver_id] = (latitude, longitude, timestamp)

        # Update spatial index
        self.location_index.update(driver_id, latitude, longitude)

        # Publish location update to interested services
        self._publish_location_update(driver_id, latitude, longitude)

    def find_nearby_drivers(self, latitude, longitude, radius_km=5):
        """Find drivers within radius"""
        return self.location_index.search_within_radius(
            latitude, longitude, radius_km
        )

class MatchingService:
    """Matches riders with drivers"""

    def __init__(self, location_service):
        self.location_service = location_service
        self.pending_requests = {}

    def request_ride(self, rider_id, pickup_lat, pickup_lon, destination_lat, destination_lon):
        """Process ride request"""
        # 1. Find nearby drivers
        nearby_drivers = self.location_service.find_nearby_drivers(
            pickup_lat, pickup_lon, radius_km=5
        )

        # 2. Filter available drivers
        available_drivers = self._filter_available_drivers(nearby_drivers)

        # 3. Apply matching algorithm
        matched_driver = self._apply_matching_algorithm(
            available_drivers, pickup_lat, pickup_lon
        )

        if matched_driver:
            # 4. Create trip and notify both parties
            trip_id = self._create_trip(rider_id, matched_driver,
                                      pickup_lat, pickup_lon,
                                      destination_lat, destination_lon)
            return trip_id
        else:
            # No drivers available, add to waiting queue
            self.pending_requests[rider_id] = {
                'pickup_lat': pickup_lat,
                'pickup_lon': pickup_lon,
                'destination_lat': destination_lat,
                'destination_lon': destination_lon,
                'timestamp': time.time()
            }
            return None

    def _apply_matching_algorithm(self, drivers, pickup_lat, pickup_lon):
        """Apply matching algorithm (closest driver, ETA-based, etc.)"""
        if not drivers:
            return None

        # Simple: closest driver
        closest_driver = min(drivers,
                           key=lambda d: self._calculate_distance(
                               d['lat'], d['lon'], pickup_lat, pickup_lon
                           ))
        return closest_driver

class QuadTree:
    """Spatial index for efficient proximity searches"""

    def __init__(self, boundary=None, max_points=10, max_depth=5):
        self.boundary = boundary or Rectangle(-90, -180, 90, 180)
        self.max_points = max_points
        self.max_depth = max_depth
        self.points = []
        self.children = None
        self.depth = 0

    def insert(self, point):
        """Insert point into quadtree"""
        if not self.boundary.contains(point):
            return False

        if len(self.points) < self.max_points or self.depth >= self.max_depth:
            self.points.append(point)
            return True

        if self.children is None:
            self._subdivide()

        for child in self.children:
            if child.insert(point):
                return True

        return False

    def search_within_radius(self, lat, lon, radius_km):
        """Search for points within radius"""
        result = []
        search_boundary = self._create_search_boundary(lat, lon, radius_km)
        self._search_recursive(search_boundary, result, lat, lon, radius_km)
        return result

    def _search_recursive(self, boundary, result, center_lat, center_lon, radius_km):
        """Recursive search in quadtree"""
        if not self.boundary.intersects(boundary):
            return

        for point in self.points:
            distance = self._calculate_distance(
                point.lat, point.lon, center_lat, center_lon
            )
            if distance <= radius_km:
                result.append(point)

        if self.children:
            for child in self.children:
                child._search_recursive(boundary, result, center_lat, center_lon, radius_km)
```

## Summary

This intermediate HLSD guide covers:

1. **Distributed Systems**: Challenges, consensus algorithms, failure handling
2. **Data Partitioning**: Sharding strategies, consistent hashing, rebalancing
3. **Replication**: Master-slave, master-master, conflict resolution
4. **Consistency**: Strong, eventual, causal consistency models
5. **Message Queues**: Pub/sub, event streaming, delivery guarantees
6. **Service Discovery**: Client-side, server-side, health checking
7. **API Gateway**: Routing, middleware, service mesh patterns
8. **Distributed Caching**: Invalidation, warming strategies
9. **Search & Indexing**: Full-text search, faceted search, spatial indexing
10. **Real-world Examples**: Chat systems, ride-sharing platforms

These concepts enable building robust, scalable distributed systems that can handle complex real-world requirements while maintaining performance, consistency, and reliability.